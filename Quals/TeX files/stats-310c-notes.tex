\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,amssymb,graphicx,mathtools,flexisym, float, enumitem}
\usepackage{accents}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\arabic{page}}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thetable}{\arabic{table}}

%
% Macros for shortcuts
%
\newcommand{\dis}{\displaystyle}
\def\lc{\left\lceil}   
\def\rc{\right\rceil}
\def\lf{\left\lfloor}   
\def\rf{\right\rfloor}
\newcommand\bbC{\mathbb{C}}
\newcommand\bbE{\mathbb{E}}
\newcommand\bbI{\mathbb{I}}
\newcommand\bbN{\mathbb{N}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbQ{\mathbb{Q}}
\newcommand\bbR{\mathbb{R}}
\newcommand\bbS{\mathbb{S}}
\newcommand\bbT{\mathbb{T}}
\newcommand\bbZ{\mathbb{Z}}
\newcommand\calA{\mathcal{A}}
\newcommand\calB{\mathcal{B}}
\newcommand\calC{\mathcal{C}}
\newcommand\calF{\mathcal{F}}
\newcommand\calG{\mathcal{G}}
\newcommand\calH{\mathcal{H}}
\newcommand\calL{\mathcal{L}}
\newcommand\calM{\mathcal{M}}
\newcommand\calN{\mathcal{N}}
\newcommand\calP{\mathcal{P}}
\newcommand\calR{\mathcal{R}}
\newcommand\calS{\mathcal{S}}
\newcommand\calU{\mathcal{U}}
\newcommand\calX{\mathcal{X}}
\newcommand\calY{\mathcal{Y}}
\newcommand\dlt{\delta}
\newcommand\Dlt{\Delta}
\def\eps{\varepsilon}
\newcommand\lmb{\lambda}
\newcommand\Lmb{\Lambda}
\newcommand\om{\omega}
\newcommand\Om{\Omega}
\newcommand\sg{\sigma}
\newcommand\Sg{\Sigma}
\def\t{\theta}
\newcommand\T{\Theta}
\newcommand\vp{\varphi}
\newcommand\equivto{\Leftrightarrow}
\newcommand\goesto{\rightarrow}
\newcommand\var{\text{Var }}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

\begin{document}
\title{STATS 310C Notes}
\author{Kenneth Tay}
\date{\vspace{-3ex}}
\maketitle

% Conditional expectation
\section*{Section 7.1: Definition, Canonical Construction and Law}
\begin{itemize}
\item (Dfn 7.1.2) \textbf{Finite-dimensional distributions (f.d.d.)} of a stochastic process $\{ X_t: t \in \bbT \}$ is the collection of joint laws of $X_{t_1}, \dots, X_{t_n}$ for all $n \in \bbN$, $t_1, \dots, t_n$ distinct values in $\bbT$.

\item (Dfn 7.1.3) A collection of f.d.d. is \textbf{consistent} if for any $B_k \in \calB$, distinct $t_k \in \bbT$ and finite $n$, $\mu_{t_1, \dots, t_n}(B_1 \times \dots \times B_n) = \mu_{t_{\pi(1)}, \dots, t_{\pi(n)}}(B_{\pi(1)} \times \dots \times B_{\pi(n)})$ for all permutations $\pi$ of $\{1, \dots, n\}$, and $\mu_{t_1, \dots, t_{n-1}}(B_1 \times \dots \times B_{n-1}) = \mu_{t_1, \dots, t_n}(B_1 \times \dots \times B_{n-1} \times \bbR)$.

\item (Dfn 7.1.5) $\bbR^\bbT := \{ \text{collection of all functions } x(t): \bbT \mapsto \bbR \}$. A \textbf{finite-dimensional rectangle} in $\bbR^\bbT$ is of the form $\{ x(\cdot): x(t_i) \in B_i, i = 1, \dots, n \}$. The \textbf{cylindrical $\sg$-algebra} $\calB^\bbT$ is the $\sg$-algebra generated by the finite-dimensional rectangles.

\item $\calB_c$ is $\calB^\bbT$ when $\bbT = \{1, 2, \dots \}$.

\item (Dfn 7.1.6) $A \subseteq \bbR^\bbT$ has \textbf{countable representation} if $A = \{ x(\cdot) \in \bbR^\bbT: (x(t_1), x(t_2), \dots) \in D \}$ for some $D \in \calB_c$ and $\{ t_1, t_2, \dots \} \subseteq \bbT$.

\item (Lem 7.1.7) $A \in \calB^\bbT$ iff $A$ has countable representation.

\item (Lem 7.1.8) \textbf{Consistent f.d.d. $\implies$ $\exists$ stochastic process with that f.d.d.:} For any consistent collection of f.d.d., there is a probability space and a stochastic process on it with that f.d.d.

\item (Dfn 7.1.9) The \textbf{law} of a stochastic process is the probability measure $\calP_X$ on $\calB^\bbT$ such that for all $A \in \calB^\bbT$, $\calP_X (A) = \bbP \{ \om: X_\cdot (\om) \in A \}$.

\item (Ex 7.1.12, HW1) A continuous time stochastic process $\{X_t: t\geq0\}$ has \textbf{independent increments} if $X_{t+h} - X_t$ is independent of $\sg(X_s: 0 \leq s \leq t)$ for all $h > 0$, $t \geq 0$.

If for all $0 \leq t_1 < t_2 < \dots < t_n < \infty$, $X_{t_1}, X_{t_2} - X_{t_1}, \dots, X_{t_n} - X_{t_{n-1}}$ are mutually independent, then $\{X_t\}$ has independent increments. (Thus, this property is determined by the f.d.d.)
\end{itemize}

\section*{Section 7.2: Continuous and Separable Modifications}
\begin{itemize}
\item (Dfn 7.2.2) Two stochastic processes $\{X_t\}$ and $\{Y_t\}$ are \textbf{versions} of each other if they have the same f.d.d. They are \textbf{modifications} if $\bbP(X_t \neq Y_t) = 0$ for all $t$. They are \textbf{indistinguishable} if $\{\om: X_t(\om) \neq Y_t(\om) \text{ for some } t \in \bbT \}$ is a $\bbP$-null set.

Modifications are versions, but converse fails.

\item (Ex 7.2.3, HW1) \textbf{Right-continuous modification is unique:} Modifications which both have w.p. 1 right-continuous sample functions are indistinguishable. Hence, we can talk of \textbf{the} right-continuous modification.

\item (Dfn 7.2.5) A function $f$ on metric space $(T, d)$ is \textbf{locally $\gamma$-H\"{o}lder continuous} if
\[ \sup_{\{ t \neq s, d(t, u) \vee d(s, u) < h_u \}} \frac{|f(t) - f(s)|}{d(t,s)^\gamma} \leq c_u, \]
for some $\gamma > 0$, some $c : \bbT \mapsto [0, \infty)$, and $h: \bbT \mapsto (0, \infty]$. It is \textbf{uniformly $\gamma$-H\"{o}lder continuous} if the same applies for constant $c < h = \infty$.

A stochastic process is locally/uniformly $\gamma$-H\"{o}lder continuous if its sample functions have that property.

Local $\gamma$-H\"{o}lder continuity implies continuity.

\item (Thm 7.2.6) \textbf{Kolmogorov-Centsov continuity theorem:} Suppose $\{X_t: t \in \bbT\}$ with $\bbT = \bbI^r$, where $\bbI$ is a compact interval. If there exist positive constants $\alpha$, $\beta$ and finite $c$ such that
\[ \bbE [|X_t - X_s|^\alpha] \leq c\|t-s\|^{r + \beta} \] for all $s, t \in \bbT$, then $\{X_t\}$ has a continuous modification which is locally $\gamma$-H\"{o}lder continuous for any $0 < \gamma < \beta/\alpha$.

\item (Lemma 7.2.12) While the above provides a continuous modification on compact intervals, the proof can be modified to get one continuous modification valid on $[0, \infty)$.

\item (Ex 7.2.13, HW1) A simpler condition than Kolmogorov-Centsov to show H\"{o}lder continuity.

\item (Dfn 7.2.15) A function $x \in \bbR^\bbI$ is \textbf{$\bbC$-separable} if $\bbC$ is countable and for any $t$, there is a sequence $\{s_k\} \subseteq \bbC$ that converges to $t$ such that $x(s_k) \goesto x(t)$. A continuous-time stochastic process is \textbf{$\bbC$-separable} if its sample functions are $\bbC$-separable.

\item (Prop 7.2.16) \textbf{Can always work with a separable process:} Any continuous time stochastic process $\{ X_t: t \in \bbI\}$ has a separable modification (consisting possibly of $\bar{\bbR}$-valued variables). (But note that separability does not imply measurability.)

\item (Dfn 7.2.19) A stochastic process $\{X_t, t \in \bbI\}$ is \textbf{measurable} if $(t, \om) \mapsto X_t(\om)$ is measurable w.r.t. the joint $\sg$-algebra.

\item (Dfn 7.2.20) A stochastic process $\{ X_t, t \in \bbI\}$ is \textbf{continuous in probability} if for any $t \in \bbI$ and $\eps > 0$, $\dis\lim_{s \goesto t} \bbP (|X_s - X_t| > \eps) = 0$. This is a very mild condition which is determined by the f.d.d.

\item (Prop 7.2.21) Any continuous in probability process has a separable modification (consisting possibly of $\bar{\bbR}$-valued variables) which is a measurable process.

\end{itemize}

\section*{Section 7.3: Gaussian and stationary processes}
\begin{itemize}
\item (Dfn 7.3.1) A stochastic process is a \textbf{Gaussian process} iff it has multivariate normal f.d.d. A Gaussian process is \textbf{centered} if $\bbE [X_t] = 0$ for all $t$.

\item (Dfn 7.3.2) A symmetric function $c$ on a product set $\bbT \times \bbT$ is \textbf{non-negative definite/positive semidefinite} if for any finite $n$ and $t_k \in \bbT$ and for any $a_k \in \bbR$, $\dis\sum_{j=1}^n\sum_{k=1}^n a_j c(t_j, t_k) a_k \geq 0$.

\item (Eg 7.3.3) The \textbf{auto-covariance function} $c(s,t) = \text{Cov}(X_s, X_t)$ of a square-integrable stochastic process is non-negative definite.

\item (Ex 7.3.4, HW2) The law of a Gaussian process is uniquely determined by its mean and auto-covariance functions. A Gaussian process exists for any mean function and PSD auto-covariance function.

\item (Prop 7.3.5) Gaussian processes are closed w.r.t. $L^2$: i.e. if $\bbE [(X_t - X_t^{(k)})^2] \goesto 0$ as $k \goesto \infty$ for each fixed $t$, then $X_t$ is also a Gaussian process, whose mean and auto-covariance functions are the pointwise limits of those for $X_t^{(k)}$.

\item (Cor 7.3.6) \textbf{Condition for Gaussian SP to have indep increments:} A continuous time Gaussian stochastic process has independent increments iff $\text{Cov}(Y_t - Y_u, Y_s) = 0$ for all $s \leq u < t$. Equivalently, the Gaussian process has auto-covariance function of the form $c(t, s) = g(t \wedge s)$.

\item (Dfn 7.3.7) A continuous time stochastic process is \textbf{stationary} if its law is invariant under any time shift $\t_s$, $s \geq 0$, or equivalently, $(X_{t_1}, \dots, X_{t_n}) \stackrel{d}{=} (X_{t_1 + s}, \dots, X_{t_n + s})$ for all $s, t_1, \dots, t_n$.

\item (Dfn 7.3.8) A square-integrable continuous time stochastic process of constant mean function and auto-covariance function of the form $c(t,s) = r(|t-s|)$ is \textbf{weakly stationary}.

Any square-integrable stationary process is weakly stationary.

\item (Ex 7.3.9, HW2) Any weakly stationary Gaussian process is also stationary. (Not true for general processes.)

\item (Ex 7.3.10, HW2) Any centered, weakly stationary process of independent increments must be a modification of the trivial process (i.e. constant sample functions).

\item (Dfn 7.3.11) A continuous time stochastic process $\{X_t\}$ has \textbf{stationary increments} if the law of $X_t - X_s$ depends only on $t-s$.

\item (Ex 7.3.14) Every stationary process has stationary increments.

\end{itemize}

\section*{Section 8.1: Continuous time filtrations and stopping times}
\begin{itemize}
\item (Dfn 8.1.4) \textbf{Left filtration} $\calF_{t^-} = \sg(\calF_s, s < t)$, \textbf{right filtration} $\calF_{t^+} = \dis\bigcap_{\eps > 0} \calF_{t + \eps}$. A filtration $\{ \calF_t\}$ is \textbf{right-continuous} if $\calF_t = \calF_{t^+}$ for all $t \geq 0$.

\item (Eg 8.1.5) \textbf{Interpolated filtration} $\calF_t = \calG_{\lf t \rf}$. Any interpolated filtration is right-continuous, but usually not left-continuous.

\item (Eg 8.1.6) Sample path continuity for $\{ X_t\}$ does not guarantee right-continuity of canonical filtration $\{ \calF_t^X\}$.

\item (Dfn 8.1.7) An $\calF_t$-adapted $\{ X_t \}$ is \textbf{$\calF_t$-progressively measurable} if $X_s(\om): [0,t] \times \Om \mapsto \bbR$ is measurable w.r.t. $\calB_{[0,t]} \times \calF_t$ for each $t \geq 0$.

\item (Prop 8.1.8) \textbf{Adapted + right-continuous  $\implies$ progressively measurable:} An $\calF_t$-adapted stochastic process with right-continuous sample functions is also $\calF_t$-progressively measurable.

\item (Dfn 8.1.9) $\tau: \Om \mapsto [0, \infty]$ is a \textbf{$\calF_t$-stopping time} if $\{\tau \leq t\} \in \calF_t$ for all $t \geq 0$. The \textbf{stopped $\sg$-algebra} is $\calF_{\tau} := \{ A \in \calF_\infty: A \cap \{ \tau \leq t\} \in \calF_t \text{ for all } t \geq 0 \}$.

$\calF_{t^+}$-stopping times are \textbf{Markov times/optional times}.

Every stopping time is a Markov time. The 2 ideas are the same for right-continuous filtrations.

\item (Ex 8.1.10, HW2) $\tau$ is an $\calF_t$-Markov time iff $\{\tau < t\} \in \calF_t$ for all $t \geq 0$.

\item (Ex 8.1.10, HW2) If $\tau_1, \tau_2, \dots$ are $\calF_t$-stopping times, then so are $\tau_1 \wedge \tau_2$, $\tau_1 + \tau_2$ and $\sup_n \tau_n$. If they are $\calF_t$-Markov times, then \emph{in addition} $\inf_n \tau_n$, $\liminf_n \tau_n$ and $\limsup_n \tau_n$ are $\calF_t$-Markov times.

\item (Ex 8.1.10d, HW2) If $\tau_1$ and $\tau_2$ are $\calF_t$-Markov times, then $\tau_1 + \tau_2$ is an $\calF_t$-stopping time when either both are strictly positive, or alternatively when $\tau_1$ is a strictly positive $\calF_t$-stopping time.

\item (Ex 8.1.11, HW3) Suppose $\t$ and $\tau$ are $\calF_t$-stopping times. Then
\begin{itemize}
\item $\sg(\tau) \subseteq \calF_\tau$, $\calF_\tau$ is a $\sg$-algebra. If $\tau = t$ non-random, then $\calF_\tau = \calF_t$.

\item $\calF_{\t \wedge \tau} = \calF_\t \cap \calF_\tau$. The events $\{\t < \tau\}$, $\{\t \leq \tau\}$ and $\{\t = \tau \}$ all belong to $\calF_{\t \wedge \tau}$.

\item For any integrable random variable $Z$, $\bbE [\bbE[Z \mid \calF_\t] \mid \calF_\tau] = \bbE[Z \mid \calF_{\t \wedge \tau}]$.

\item If $\t \leq \xi$ and $\xi \in m\calF_\t$, then $\xi$ is an $\calF_t$-stopping time.
\end{itemize}

\item (Prop 8.1.13) If $\{X_s\}$ is $\calF_t$-progressively measurable, then for any $\calF_t$-stopping time $\tau$, $\{X_{s \wedge \tau}\}$ is also $\calF_t$-progressively measurable. (Not true for adaptedness.)

In particular, if $\tau < \infty$ or there exists $X_\infty \in m\calF_\infty$, then $X_\tau \in m\calF_\tau$.

\item (Prop 8.1.15) Let $\{ X_s, s \geq 0\}$ be an $\calF_t$-adapted right-continuous stochastic process. The \textbf{first hitting time} $\tau_B(\om) := \inf \{ t \geq 0: X_t(\om) \in B\}$ is an $\calF_t$-Markov time for an open set $B$. If $B$ is a closed set and $\{X_s\}$ has continuous sample functions, then $\tau_B$ is an $\calF_t$-stopping time.

\item (Prop 8.1.16) Given an $\calF_t$-Markov time $\tau$, there is a decreasing sequence of $\calF_t$-stopping times $\tau_\ell$ such that $\tau_\ell \downarrow \tau$ and $\tau_\ell$ is $\bbQ^{(2, \ell)}$-valued.
\end{itemize}

\section*{Section 8.2: Continuous time martingales}
\begin{itemize}
\item (Ex 8.2.2) If $\{X_t\}$ is a submartingale with $\bbE X_t = \bbE X_0$ for all $t \geq 0$, then it is also a martingale.

\item (Ex 8.2.3) If $\{ X_t\}$ is a square-integrable martingale, then $\bbE [(X_t - X_s)^2 \mid \calF_s] = \bbE [X_t^2 \mid \calF_s] - X_s^2$ for all $0 \leq s \leq t$, which implies that $t \mapsto \bbE X_t^2$ is non-decreasing.

\item (Prop 8.2.4) Any integrable stochastic process $\{X_t, t\geq0\}$ of independent increments and constant mean function is a martingale.

\item (Ex 8.2.6, HW3) If $\{X_t\}$ is square-integrable, having zero-mean independent increments, then $X_t^2 - \langle X \rangle_t$ is a martingale, where $\langle X \rangle_t = \bbE X_t^2 - \bbE X_0^2$ is non-random and non-decreasing.

\item (Ex 8.2.6, HW3) If $\{X_t\}$ is square-integrable with $X_0 = 0$ and zero-mean, stationary independent increments, then $X_t^2 - t\bbE X_1^2$ is a martingale.

\item (Thm 8.2.14) \textbf{Doob's inequality:} If $\{X_s\}$ is a right-continuous submartingale, then for $t \geq 0$ finite, $M_t = \dis\sup_{0 \leq s \leq t} X_s$ and any $x > 0$,
\[ \bbP (M_t \geq x) \leq \frac{1}{x} \bbE [X_t 1_{\{M_t \geq x\}}] \leq \frac{1}{x} \bbE [(X_t)_+]. \]

\item (Ex 8.2.15) For $p > 1$, in the setting above, $\bbP (M_t \geq x) \leq \dfrac{1}{x^p} \bbE [(X_t)_+^p]$. If $\{X_t\}$ is a right-continuous martingale, we have $\bbP \left(\dis\sup_{0 \leq s \leq t} |X_s| \geq x \right) \leq \dfrac{1}{x^p} \bbE [|X_t|^p]$.

\item (Cor 8.2.16) \textbf{$L^p$ maximal inequalities:} For any $p > $, $t \geq 0$ and right-continuous submartingale $\{X_t\}$, we have $\bbE \left[ \left(\dis\sup_{0 \leq u \leq t} X_u \right)_+^p \right] \leq q^p \bbE [(X_t)_+^p]$, where $q = p/(p-1)$ (i.e. $\frac{1}{p}+\frac{1}{q} = 1$).

If $\{X_t\}$ is a right-continuous martingale, we have $\bbE \left[ \left(\dis\sup_{0 \leq u \leq t} |X_u| \right)^p \right] \leq q^p \bbE [|X_t|^p]$.

\item (Thm 8.2.20) \textbf{Doob's Convergence Theorem:} Suppose right-continuous supermartingale $\{X_t\}$ is such that $\sup_t \bbE [(X_t)_-] < \infty$. Then $X_t \stackrel{a.s.}{\goesto} X_\infty$ and $\bbE |X_\infty| \leq \liminf_t \bbE |X_t|$ is finite.

\item (Dfn 8.2.22) A submartingale $\{X_t\}$ is \textbf{right closable}, or has a \textbf{last element} $X_\infty$ if $X_\infty \in L^1(\Om, \calF_\infty, P)$ is such that for any $t \geq 0$, $\bbE [X_\infty \mid \calF_t] \geq X_t$ a.s.

For a supermartingale, we require $\bbE [X_\infty \mid \calF_t] \leq X_t$ a.s. For a martingale, we require $\bbE [X_\infty \mid \calF_t] = X_t$ a.s., i.e. $\{ X_t\}$ is a Doob martingale of $X_\infty$ w.r.t. $\{ \calF_t\}$.

\item (Prop 8.2.23) For a right-continuous non-negative submartingale $\{X_t\}$, the following are equivalent:
\begin{enumerate}[label = (\alph*)]
\item $\{X_t\}$ is U.I.
\item $X_t \goesto X_\infty$ in $L^1$.
\item $X_t \stackrel{a.s.}{\goesto} X_\infty$ a last element of $\{ X_t\}$.
\end{enumerate}
Without non-negativity, we still have $(a) \Leftrightarrow (b) \Rightarrow (c)$. A right-continuous martingale has all these properties iff it is a Doob martingale.

\item (Prop 8.2.24) \textbf{Doob's $L^p$ Martingale Convergence:} If right-continuous martingale $\{X_t\}$ is $L^p$-bounded for some $p > 1$, then $X_t \goesto X_\infty$ a.s. and in $L^p$.

\item (Thm 8.2.25) Suppose $(X_t, \calF_t)$ is a supermartingale with right-continuous filtration and $t \mapsto \bbE X_t$ is right-continuous. Then there exists an RCLL modification $\tilde{X}_t$ of $X_t$ such that $(\tilde{X}_t, \calF_t)$ is a supermartingale.

\item (Thm 8.2.26) \textbf{Doob's Optional Stopping Theorem:} If $(X_t, \calF_t)$ is a right-continuous submartingale with a last element $(X_\infty, \calF_\infty)$, then for any $\calF_t$-Markov times $\tau \geq \t$, $X_\t$ and $X_\tau$ are integrable and $\bbE [X_\tau] \geq \bbE [X_\t]$, with equality in the case of a martingale.

\item (Cor 8.2.27) If $(X_t, \calF_t)$ is a right-continuous submartingale with a last element $(X_\infty, \calF_\infty)$, then for any $\calF_t$-Markov times $\tau \geq \t$, $\bbE [X_\tau \mid \calF_{\t^+}] \geq X_\t$ w.p. 1 (with equality in the case of a martingale).

If $\t$ is a $\calF_t$-stopping time, then we also have $\bbE [X_\tau \mid \calF_\t] \geq X_\t$ w.p. 1 (with equality in the case of a martingale).

\item (Remark 8.2.28) In Thm 8.2.26 and Cor 8.2.27, if $\tau$ is a bounded Markov time, then we don't need the existence of a last element for the statements to be true.

\item (Cor 8.2.29) \textbf{Stopped processes:} If $\eta$ is an $\calF_t$-stopping time and $(X_t, \calF_t)$ is a right-continuous submartingale, then $\{ X_{t \wedge \eta}\}$ is also a right-continuous submartingale.

\item (Ex 8.2.30, HW4) \textbf{Optional Stopping Theorem for stopped processes:} If $(X_t, \calF_t)$ is a right-continuous submartingale and $u \geq 0$ a non-random constant, then for any $\calF_t$-stopping times $\tau \geq \t$, we have $\bbE [X_{u \wedge \tau} \mid \calF_\t] \geq X_{u \wedge \t}$ w.p. 1 (equality for a martingale). This implies that $\bbE [X_{u \wedge \tau}] \geq \bbE [X_{u \wedge \t}]$ (equality for martingale).

If $X_{u \wedge \tau}$ is U.I., then we can also conclude that $X_\t$ and $X_\tau$ are integrable and $\bbE [X_\tau \mid \calF_\t] \geq X_\t$ a.s. (equality for martingale).

\item (Dfn 8.2.37) The \textbf{standard $d$-dimensional Brownian motion} is the $\bbR^d$-valued stochastic process such that its $d$ components are mutually independent, standard one-dimensional Wiener processes. It is a martingale and a centered Gaussian process of continuous sample functions and stationary, independent increments.

\item (Dfn 8.2.39) Fix a right-continuous filtration $\{\calF_t\}$. $\calM_2 :=$ the vector space of all square integrable $\{\calF_t\}$-martingales which have $X_0 = 0$ and right-continuous sample functions.

$\calM_2^c :=$ the linear subspace of $\calM_2$ with continuous sample functions.

\item (Dfn 8.2.40) An \textbf{$\{\calF_t\}$-increasing process} is an $\calF_t$-adapted, integrable stochastic process $\{ A_t\}$ of right-continuous, non-decreasing sample functions starting at $A_0 = 0$.

\item (Dfn 8.2.41) \textbf{$q^{th}$ variation:} For any finite partition $\pi = \left\{ a = s_0^{(\pi)} < \dots < s_k^{(\pi)} = b \right\}$ of $[a,b]$, let $\| \pi \|$ denote the length of the longest interval in $\pi$, and let the \textbf{$q^{th}$ variation of $f$ on partition $\pi$} be
\[ V_{(\pi)}^{(q)} (f) = \sum_{i=1}^k \left|f(s_i^{(\pi)}) - f(s_{i-1}^{(\pi)}) \right|^q.\]

The \textbf{$q^{th}$ variation of $f$ on $[a,b]$} is $V^{(q)}(f) = \dis\lim_{\|\pi\| \goesto 0} V_{(\pi)}^{(q)} (f)$.

\item (Lem 8.2.43) If a martingale of continuous sample functions has finite total variation on each compact interval, then it is indistinguishable from a constant.

\item (Lem 8.2.44) Suppose $X \in \calM_2^c$. For any partition $\pi = \{ 0 = s_0 < s_1 < \dots \}$ of $[0 ,\infty)$ with a finite number of points on each compact interval, $M_t^{(\pi)} = X_t^2 - V_t^{(\pi)}(X)$ is an $\calF_t$-martingale of continuous sample paths, where
\[ V_t^{(\pi)}(X) := \sum_{i=1}^k (X_{s_i} - X_{s_{i-1}})^2 + (X_t - X_{s_k})^2, \qquad \text{for all } t \in [s_k, s_{k+1}). \]

\item (Thm 8.2.45) \textbf{Special case of Doob-Meyer Decomposition:} For $X \in \calM_2^c$, the continuous modification of $V^{(2)}(X)_t$ is the unique $\calF_t$-increasing process $A_t = \langle X \rangle_t$ of continuous sample paths, such that $M_t = X_t^2 - A_t$ is an $\calF_t$-martingale of continuous sample paths. Any two such decompositions of $X_t^2$ as the sum of a martingale and increasing process are indistinguishable.

\item (Ex 8.2.46, HW8) Suppose $\{ X_t\}$ of continuous sample paths has an a.s. finite $r^{th}$ variation for each fixed $t > 0$. Then for each $t > 0$, $V^{(q)}(X)_t \stackrel{a.s.}{=} 0$ if $q > r$. If $0 < q < r$, $V^{(q)}(X)_t \stackrel{a.s.}{=} \infty$ a.e. $\om$ such that $V^{(r)}(X)_t > 0$.

\item (Ex 8.2.46, HW8) If $X \in \calM_2^c$ and $\widetilde{A}_t$ has continuous sample paths and finite total variation on compact intervals, then the quadratic variation of $X_t + \widetilde{A}_t$ is $\langle X \rangle_t$.

\item (Ex 8.2.46, HW8) If $\{ X_t\}$ is locally $\gamma$-H\"{o}lder continuous on $[0,T]$ for some $\gamma > 1/2$, then its quadratic variation on this interval is 0.

\item (Dfn 8.2.47) An $\calF_t$-progressively measurable stochastic process $\{Y_t \}$ is of \textbf{class DL} if the collection $\{ Y_{u \wedge \t}: \t \text{ an } \calF_t\text{-stopping time}\}$ is U.I. for each finite, non-random $u$.
\begin{itemize}
\item (Ex 8.2.50) Every non-negative right-continuous submartingale is of class DL.
\end{itemize}

\item (Thm 8.2.48) \textbf{Doob-Meyer Decomposition:} A right-continuous $\calF_t$-submartingale $\{Y_t\}$ admits the decomposition $Y_t = M_t + A_t$ with $M_t$ a continuous $\calF_t$-martingale and $A_t$ an $\calF_t$-increasing process, iff $\{ Y_t\}$ is of class DL.
\begin{itemize}
\item For uniqueness, we must require $A_t$ to be a natural process. Every continuous increasing process is a natural process, and a natural process is an increasing process.
\end{itemize}

\item (Remark) For each $X \in \calM_2$, we can associate with it a unique natural process $\langle X \rangle_t$, called the \textbf{predictable quadratic variation}, such that $X_t^2 - \langle X \rangle_t$ is a right-continuous martingale. When $X \notin \calM_2^c$, it is no longer the case that the predictable quadratic variation must match the quadratic variation.

\item (Eg 8.2.53) \textbf{L\'{e}vy's martingale characterization of Brownian motion:} Any $X \in \calM_2^c$ of quadratic variation $\langle X \rangle_t = t$ must be a standard Brownian Markov process.

\item (Eg 8.2.53) For any square integrable stochastic process with $X_0 = 0$ and zero-mean, stationary and independent increments, we have $\langle X \rangle_t = t \bbE [X_1^2]$.

\item (Dfn 8.2.56) For any pair $X, Y \in \calM_2$, the \textbf{bracket of $X$ and $Y$} is $\langle X, Y \rangle_t := \dfrac{1}{4} \left[\langle X + Y \rangle_t  - \langle X - Y \rangle_t \right]$. $X$ and $Y$ are \textbf{orthogonal} if for any $t \geq 0$, $\langle X, Y \rangle_t = 0$ a.s.

\item (Ex 8.2.57, HW9) \textbf{Properties of bracket:}
\begin{itemize}
\item $XY - \langle X, Y \rangle$ is a martingale.
\item $\langle c_1X_1 + c_2 X_2, Y \rangle = c_1\langle X_1, Y \rangle + c_2\langle X_2, Y \rangle$.
\item $\langle X, Y \rangle = \langle Y, X \rangle$.
\item $|\langle X, Y \rangle|^2 \leq \langle X \rangle \langle Y \rangle$.
\item $V(\langle X, Y \rangle)_t - V(\langle X, Y \rangle)_s \leq \dfrac{1}{2}[\langle X \rangle_t - \langle X \rangle_s + \langle Y \rangle_t - \langle Y \rangle_s]$ for all $0 \leq s < t < \infty$.
\end{itemize}

\end{itemize}

\section*{Section 8.3: Markov and Strong Markov processes}
\begin{itemize}
\item (Dfn 8.3.1) A collection $\{p_{s,t}(\cdot, \cdot), t \geq s \geq 0\}$ of \textbf{transition probabilities} on a measurable space $(\bbS, \calS)$ is \textbf{consistent} if it satisfies the \textbf{Chapman-Kolmogorov equations} 
\[ p_{t_1, t_3}(x,B) = p_{t_1, t_2}p_{t_2, t_3}(x,B) = \int p_{t_1, t_2}(x, dy)p_{t_2, t_3}(y,B) \qquad\text{for all } x \in \bbS, B \in \calS, t_3 \geq t_2 \geq t_1 \geq 0. \]

This collection is a \textbf{Markov semi-group} if $p_{s,t} = p_{t-s}$ for all $t \geq s \geq 0$.

\item (Dfn 8.3.1) An $\calF_t$-adapted $\{ X_t \}$ taking values in $(\bbS, \calS)$ is an \textbf{$\calF_t$-Markov process} if for any $t \geq s \geq 0$ and $B \in \calS$, $\bbP (X_t \in B \mid \calF_s) \stackrel{a.s.}{=} p_{s,t}(X_s, B)$.

It is a \textbf{homogeneous $\calF_t$-Markov process of semi-group $\{ p_u\}$} if for any $u, s \geq 0$ and $B \in \calS$, $\bbP (X_{s+u} \in B \mid \calF_s) \stackrel{a.s.}{=} p_u(X_s, B)$.

\item Define the map $f \mapsto (p_{s,t}f): b\calS \mapsto b\calS$ by $(p_{s,t}f)(x) = \dis\int p_{s,t}(x, dy) f(y)$ for each $x$. Then for $t \geq s \geq 0$, $\bbE [f(X_t) \mid \calF_s] = (p_{s,t}f)(X_s)$.

\item (Thm 8.3.2) Given consistent transition probabilities, you can get a Markov process with those transitions. The law of a Markov process is just its f.d.d. We use $\bbP_x$ to denote the law of the Markov process given that it starts at non-random $x$.

\item (Ex 8.3.4, HW4) \textbf{Closure of Markov processes (useful for showing that a transformation of a Markov process is still Markov:} Let $(X_t, \calF_t^X)$ be a Markov process on state space $(\bbS, \calS)$. Let $u: [0,\infty) \mapsto [0,\infty)$ be an invertible strictly increasing function. For each $t$, let $\Phi_t: (\bbS, \calS) \mapsto (\tilde{\bbS}, \tilde{\calS})$ be an invertible measurable mapping such that the inverse is measurable as well.
\begin{itemize}
\item If $Y_t = \Phi_t(X_{u(t)})$, then $(Y_t, \calF_t^Y)$ is a Markov process on state space $(\tilde{\bbS}, \tilde{\calS})$.
\item If $X_t$ is a homogeneous Markov process, then so is $Z_t = \Phi_0(X_t)$.
\end{itemize}

\item (Prop 8.3.5) \textbf{Independent increments $\implies$ Markov process:} If a real-valued $\{X_t\}$ has independent increments, then $(X_t, \calF_t^X)$ is a Markov process with transition probabilities $p_{s,t}(y, B) = P_{X_t - X_s}(\{ z: y+z \in B\})$. If $\{ X_t\}$ has stationary independent increments, then this Markov process is homogeneous.

\item (Remark) \textbf{Generator of a Markov process} is the operator $\mathbf{L} = \dis\lim_{s \downarrow 0} \dfrac{p_s - p_0}{s}$.

\item (Dfn 8.3.7) $(W_t, \calF_t)$ is a \textbf{Brownian Markov process} if it has continuous sample paths and is a homogeneous $\calF_t$-Markov process with Brownian semi-group. If in addition $W_0 = 0$, we call it a \textbf{standard Brownian Markov process}.

\item (Dfn 8.3.8) A probability measure $\nu$ on $(\bbS, \calS)$ is an \textbf{invariant measure} for a semi-group of transition probabilities $\{p_u, u \geq 0\}$, if the induced law $\bbP_\nu(\cdot) = \dis\int_\bbS P_x(\cdot) \nu(dx)$ (induced law on the space of trajectories) is invariant under any time shift $\t_s$, $s \geq 0$.

\item (Ex 8.3.9, HW5) A probability measure $\nu$ on $(\bbS, \calS)$ is an invariant measure for $\{p_u\}$ iff $\nu p_t = \nu$ for any $t \geq 0$.

\item (Prop 8.3.11) \textbf{Markov Property:} Suppose $\{X_t\}$ is a homogeneous $\calF_t$-Markov process on $(\bbS, \calS)$. Let $P_x$ denote the family of laws associated with its semi-group. Then, $x \mapsto \bbE_x[h]$ is measurable on $(\bbS, \calS)$ for any $h \in b\calS^{[0,\infty)}$, and futher for any $s \geq 0$, almost surely
\[ \bbE [h \circ \t_s (X_\cdot (\om)) \mid \calF_s] = \bbE [h \circ (X_{\cdot + s} (\om)) \mid \calF_s] = \bbE_{X_s}[h]. \]

(\textbf{Note:} If we take $h(x(\cdot)) = I_B(x(u))$, the above reduces to $\bbP(X_{s+u} \in B \mid \calF_s) = \bbP_{X_s}(X_u \in B) = p_u(X_s, B)$. In order to get the above, it suffices to check that this relation holds for all $u$, $s$ and $B$!)

\item (Dfn 8.3.13) \textbf{Strong Markov process:} An $\calF_t$-progressive measurable, homogeneous Markov process $\{ X_t\}$ on $(\bbS, \calS)$ has the \textbf{strong Markov property} if for any bounded $h(s, x(\cdot))$ measurable on the product $\sg$-algebra $\calU = \calB_{[0, \infty)} \times \calS^{[0, \infty)}$ and any $\calF_t$-Markov time $\tau$, we have (almost surely)
\[ I_{\{\tau < \infty\}} \bbE [h(\tau, X_{\tau + \cdot}(\om)) \mid \calF_{\tau^+}] = I_{\{\tau < \infty\}} g_h(\tau, X_\tau), \]
where $g_h(s,x) = \bbE_x[h(s, \cdot)]$ is bounded and measurable on $\calB_{[0,\infty)} \times \calS$.

\item (Cor 8.3.14) If $(X_t, \calF_t)$ is a strong Markov process and $\tau$ is an $\calF_t$-stopping time, then for any $h \in b\calU$, a.s.
\[ I_{\{\tau < \infty\}} \bbE [h(\tau, X_{\tau + \cdot}(\om)) \mid \calF_{\tau}] = I_{\{\tau < \infty\}} g_h(\tau, X_\tau). \]

In particular, $\{X_t\}$ is a homogeneous $\calF_{t^+}$-Markov process and for any $s \geq 0$ and $h \in b\calS^{[0, \infty)}$, $\bbE [h(X_\cdot) \mid \calF_{s^+}] = \bbE [h(X_\cdot) \mid \calF_s]$.

\item (Prop 8.3.15) \textbf{Sufficient condtion for strong Markov property:} An $\calF_t$-progressive measurable, homogeneous Markov process $\{ X_t\}$ with semi-group $\{p_u \}$ has the strong Markov property if for any $u \geq 0$, $B \in \calS$ and bounded $\calF_t$-Markov times $\tau$, we have $\bbP(X_{\tau + u} \in B \mid \calF_{\tau^+}) = p_u(X_\tau, B)$.

\item (Ex 8.3.17, HW5) If a stopping time takes on only countably many values, then any homogeneous Markov process has the strong Markov property for this stopping time.

\item (Dfn 8.3.18) A \textbf{Feller semi-group} is a Markov semi-group $\{p_u\}$ on $(\bbR, \calB)$ such that $p_t: C_b(\bbR) \mapsto C_b(\bbR)$ for any $t \geq 0$, i.e. $x \mapsto (p_tf)(x)$ is continuous for any fixed bounded, continuous function $f$ and $t \geq 0$.

\item (Prop 8.3.19) \textbf{Right-continuous + Feller $\implies$ strong Markov process:} Any right-continuous homogeneous Markov process with a Feller semi-group is a strong Markov process.

\item (Ex 8.3.20, HW5) A real-valued process of stationary and independent increments has Feller semi-group. Hence, if it is also right-continuous, then it is a strong Markov process.

\item (Eg 8.3.21) Example of a Markov process which is not a strong Markov process.

\item (Dfn 8.3.23) A function $x: \bbR_+ \mapsto \bbS$ is a \textbf{step function} if it is constant on each of the intervals $[s_{k-1}, s_k)$ for some countable (possibly finite) set of isolated points $0 = s_0 < s_1 < \dots$. (Note that a step function is right-continuous.)

A stochastic process $\{ X_t\}$ is a \textbf{pure jump process} if its sample functions are step functions.

A \textbf{Markov (pure) jump process} is a homogeneous Markov process which, starting at any non-random $X_0 = x \in \bbS$, is also a pure jump process. (Sometimes called \textbf{continuous-time Markov chains}.)

\item (Prop 8.3.24) Any Markov jump process is a strong Markov process.

\item (Eg 8.3.25) Example of a Markov jump process with non-Feller semi-group.

\item (Prop 8.3.26) Suppose $\{ X_t\}$ is a right-continuous, homogeneous Markov process.
\begin{itemize}
\item \textbf{Time to jump has exponential distribution:} There is a measurable $\lmb: \bbS \mapsto [0,\infty]$ such that for all $y \in \bbS$, under $\bbP_y$, the $\calF_t^X$-Markov time $\tau = \inf \{ t \geq 0: X_t \neq X_0\}$ has the exponential distribution of parameter $\lmb_y$. (Note: $\lmb_y$ can be $0$ or $\infty$.)

\item \textbf{When you jump is independent of where you jump to:} If $\lmb_y > 0$, then $\tau$ is $\bbP_y$-a.s. finite and $\bbP_y$-independent of the $\bbS$-valued random variable $X_\tau$.

\item If $(X_t, t \geq 0)$ is a strong Markov process and $0 < \lmb_y < \infty$, then $P_y$-a.s. $X_\tau \neq y$.

\item If $(X_t, t \geq 0)$ is a Markov jump process, then $\tau$ is a strictly positive $\calF_t^X$-stopping time.
\end{itemize}

\item (Dfn 8.3.27) For a Markov jump process $\{ X_t, t \geq 0\}$, $p(x,A)$ and $\{ \lmb_x \}$ are the \textbf{jump transition probability} and \textbf{jump rates} if 
\[p(x,A) = \begin{cases} \bbP_x(X_\tau \in A) &\text{if } A \in \calS \text{ and } x \in \bbS \text{ s.t. } \lmb_x > 0, \\ 
I\{x \in A \} &\text{if } A \in \calS \text{ and } x \in \bbS \text{ s.t. } \lmb_x = 0. \end{cases} \]

More generally, a pair $(\lmb, p)$ with $\lmb: \bbS \mapsto \bbR_+$ and $p(\cdot, \cdot)$ transition probability on $(\bbS, \calS)$ such that $p(x, \{ x\}) = I \{ \lmb_x = 0 \}$ is called \textbf{jump parameters}.

\item (Thm 8.3.28) \textbf{Canonical construction of Markov jump process:} Suppose $(\lmb, p)$ are jump parameters on a $\calB$-isomorphic space $(\bbS, \calS)$. Define
\begin{itemize}
\item $\{ Z_n, n \geq 0\}$ a homogeneous Markov chain of transition probability $p(\cdot, \cdot)$ and initial state $Z_0 = x \in \bbS$.

\item For each $y \in \bbS$, let $\{\tau_j(y) \geq 1 \}$ be i.i.d. random variables independent of $\{ Z_n\}$ and each having $\text{Exp}(\lmb_y)$ distribution.

\item $T_0 = 0$, $T_k = \dis\sum_{j=1}^k \tau_j (Z_{j-1})$ for $k \geq 1$.

\item $X_t = Z_k$ for all $t \in [T_k, T_{k+1})$, $k \geq 0$.
\end{itemize}
Assuming $\bbP_x(T_\infty < \infty) = 0$ for all $x \in \bbS$ (``non-explosion condition"), then $\{ X_t\}$ is the unique Markov jump process with the given jump parameters.

Conversely, $(\lmb, p)$ are the parameters of a Markov jump process iff $\bbP_x(T_\infty < \infty) = 0$ for all $x \in \bbS$.

\item (Remark 8.3.29) When the jump rates are all equal, i.e. $\lmb_x = \lmb$, the jump times $T_k$ are those of a Poisson process $N_t$ of rate $\lmb$, which is independent of the Markov chain $\{Z_n\}$. Hence, we can write the Markov jump process as $X_t = Z_{N_t}$.

\item (Ex 8.3.30, HW9)  Suppose $(\lmb, p)$ are jump parameters on $(\bbS, \calS)$. 
\begin{itemize}
\item \textbf{Rewriting of non-explosion condition:} $\bbP_x(T_\infty < \infty) = 0$ iff $\bbP_x \left(\sum_n \lmb_{Z_n}^{-1} < \infty \right) = 0$.

\item \textbf{Sufficient condition on jump parameters to get MJP:} If $\lmb \in b\calS$ (i.e. bounded), then the jump parameters correspond to a well-defined unique Markov jump process.
\end{itemize}

\item (Eg 8.3.31) \textbf{Birth processes:} Birth processes are Markov jump processes which are also counting processes. (Generalization of Poisson processes: jump times can depend on the state.) They have state space $\bbS = \{ 0, 1, \dots\}$ and jump transition probability $p(x, x+1) = 1$.

\item (Dfn 8.3.32) The linear operator $\mathbf{L}: b\calS \mapsto m\calS$ such that $(\mathbf{L}h)(x) = \lmb_x \dis\int (h(y) - h(x))p(x,dy) = \lmb_x \bbE_x [h(X_\tau) - h(X_0)]$ is the \textbf{generator} of the Markov jump process with parameters $(\lmb, p)$.

In particular, $(\mathbf{L}I_{\{x\}^c})(x) = \lmb_x$, and for any $B \subseteq \{ x\}^c$, $(\mathbf{L}I_B)(x) = \lmb_x p(x,B)$.

\item (Ex 8.3.33, HW9) Let $\{ X_t\}$ be a Markov jump process of semi-group $p_t(\cdot, \cdot)$ and jump parameters $(\lmb, p)$.
\begin{itemize}
\item \textbf{Kolmogorov backward equation:} $t \mapsto (\mathbf{L}p_t h)(x)$ is continuous and $t \mapsto (p_t h)(x)$ is differentiable for any $x \in \bbS$, $h \in b\calS$, $t \geq 0$. We also have $\partial_t (p_t h)(x) = (\mathbf{L}p_t h)(x)$ for all $t \geq 0, x \in \bbS, h \in b\calS$.

\item \textbf{Kolmogorov forward equation:} If $\sup_{s \in \bbS} \lmb_x$ is finite, then $\mathbf{L}: b\calS \mapsto b\calS$, and for all $t \geq 0, x \in \bbS, h \in b\calS$, we have $\partial_t (p_t h)(x) = (p_t(\mathbf{L}h))(x)$.

\item A Markov semi-group $p_t(\cdot, \cdot)$ corresponds to a Markov jump process only if for any $x \in \bbS$, the limit $\dis\lim_{t \downarrow 0}\frac{1 - p_t(x, \{ x\})}{t} = \lmb_x$ exists, is finite and $\calS$-measurable.
\end{itemize}

\item (Dfn 8.3.35) \textbf{Compound Poisson process:} A compound Poisson process is a real-valued Markov jump processes with a constant jump rate $\lmb$, whose jump transition probability is of the form $p(x, B) = P_\xi (\{ z: x + z \in B\})$ for some law $P_\xi$ on $(\bbR, \calB)$.

It is of the form $X_t = S_{N_t}$, $S_n = S_0 + \dis\sum_{k=1}^n \xi_k$ is an i.i.d. random walk which is independent of Poisson process $N_t$ with rate $\lmb$. (A random walk sampled at Poisson process times.)

\item (Prop 8.3.36) A compound Poisson process $X_t$ has stationary, independent increments and the characteristic function of its Markov semi-group $p_t(x,\cdot)$ is $\bbE_x [e^{i\t X_t}] = \exp \left[i\t x + \lmb t (\Phi_\xi(\t) - 1) \right]$, where $\Phi_\xi(\cdot)$ is the characteristic function for $\xi$.

\item (Prop 8.3.38) Partitioning of compound Poisson processes.

\end{itemize}

\section*{Section 9.1: Brownian transformations, hitting times and maxima}
\begin{itemize}
\item (Ex 9.1.1, HW5) Let $\{ W_t, t \geq 0\}$ be a standard Wiener process. The following are also standard Wiener processes:
\begin{itemize}
\item \textbf{Symmetry:} $\widetilde{W}_t^{(1)} = -W_t$.
\item \textbf{Time-homogeneity:} $\widetilde{W}_t^{(2)} = W_{T+t} - W_T$, $t \geq 0$ with $T > 0$ a non-random constant.
\item \textbf{Time-reversal:} $\widetilde{W}_t^{(3)} = W_T - W_{T-t}$ for $t \in [0,T]$, with $T > 0$ a non-random constant. Morever, $\widetilde{W}_t^{(2)}$ and $\widetilde{W}_t^{(3)}$ are independent.
\item \textbf{Scaling:} $\widetilde{W}_t^{(4)} = \alpha^{-1/2}W_{\alpha t}$, where $\alpha > 0$ is a non-random cosntant.
\item \textbf{Time-inversion:} $\widetilde{W}_t^{(5)} = tW_{1/t}$ for $t > 0$ and $\widetilde{W}_0^{(5)} = 0$.
\item \textbf{Averaging:} $\widetilde{W}_t^{(6)} = \dis\sum_{k=1}^n c_k W_t^{(k)}$, where $W_t^{(k)}$ are independent copies of the standard Wiener process and $\sum c_k^2 = 1$.
\end{itemize}

\item (Remark after Ex 9.1.1) If $L_{a,b} = \sup \{t \geq 0: W_t \notin (-at, bt) \}$, by time inversion we can show that $L_{a,b}$ is a.s. finite and $\bbP(W_{L_{a,b}} = bL_{a,b}) = a/(a+b)$.

\item (Cor 9.1.2) If $(W_t,\calF_t)$ is a Brownian Markov process, then it is a homogeneous $\calF_{t^+}$-Markov process, and for any $s \geq 0$ and bounded Borel-measurable functional $h: ([0,\infty)) \mapsto \bbR$, a.s. $\bbE [h(W_\cdot) \mid \calF_{s^+}] = \bbE [h(W_\cdot) \mid \calF_s]$.

\item (Prop 9.1.4) \textbf{Blumenthal's 0-1 Law:} Let $P_x$ denote the law of the Wiener process starting at $W_0 = x$. Then $P_x(A) \in \{ 0,1\}$ for each $A \in \calF_{0^+}^W$ and $x \in \bbR$. If $A$ is in the tail algebra for $\{ W_t\}$, then either $P_x(A) = 0$ for all $x$ or $P_x(A) = 1$ for all $x$.

\item (Cor 9.1.5) Let $\tau_{0^+} = \inf \{ t \geq 0: W_t > 0\}$, $\tau_{0^-} = \inf \{ t \geq 0: W_t < 0\}$ and $T_0 = \inf \{ t \geq 0: W_t = 0\}$. Then $P_0(\tau_{0^+} = 0) = P_0(\tau_{0^-} = 0) = P_0(T_0 = 0) = 1$, and w.p. 1 the standard Wiener process changes sign infinitely many times in any time interval $[0, \eps]$, $\eps > 0$.

\item (Cor 9.1.5) For any $x \in \bbR$, with $P_x$-probability 1,
\[ \limsup_{t \goesto \infty} \frac{W_t}{\sqrt{t}} = \infty, \quad \liminf_{t \goesto \infty} \frac{W_t}{\sqrt{t}} = -\infty, \quad W_{u_n} = 0 \text{ for some } u_n(\om) \nearrow \infty. \]

\item (Cor 9.1.6) \textbf{Regeneration:} If $(W_t, \calF_t)$ is a Brownian Markov process and $\tau$ is an a.s. finite $\calF_t$-Markov time, then $\{ W_{\tau + t} - W_\tau \}$ is a standard Wiener process independent of $\calF_{\tau^+}$.

\item (Ex 9.1.8, HW6) Let $\tau_b = \inf \{ t \geq 0: W_t \geq b\}$ and $\tau_{b^+} = \inf \{ t \geq 0: W_t > b\}$. Then $P_0(\tau_b \neq \tau_{b^+}) = 0$.

\item (Prop 9.1.10) \textbf{Reflection principle:} Let $M_t = \dis\sup_{s \in [0,t]} W_s$, $T_b = \inf \{ t \geq 0: W_t = b\}$. Then for any $t, b > 0$,
\[ P(M_t \geq b) = P(\tau_b \leq t) = P(T_b \leq t) = 2P(W_t \geq b). \]
Further,
\[ f_{T_b}(t) = \frac{b}{\sqrt{2\pi t^3}} e^{-b^2/2t} = \frac{b}{\sqrt{t^3}} \phi \left(\frac{b}{\sqrt{t}}\right), \qquad f_{M_t}(b) = \frac{2}{\sqrt{2\pi t}}e^{-b^2/2t}. \]

\item (Ex 9.1.12) For any $u > 0$ and $a_1 < a_2 \leq b$, $\bbP(T_b < u, a_1 < W_u < a_2) = \bbP(2b - a_2 < W_u < 2b-a_1)$.

\item (Ex 9.1.12) \textbf{Joint density of $(M_t, W_t)$} is given by $f_{W_t, M_t}(a,b) = \dfrac{2(2b-a)}{\sqrt{2\pi t^3}} \exp \left[ -\dfrac{(2b-a)^2}{2t} \right]$.

\item $M_t = \dis\max_{0 \leq s \leq t} W_s$ has the same distribution as $\sqrt{t}|Z|$.

\item The processes $\{ M_t - W_t\}$ and $\{ |W_t| \}$ have the same distributions.

\item (Ex 9.1.15, HW6) \textbf{Brownian motion absorbed at zero} $\{ W_{t \wedge T_0}, \calF_t\}$ is a homogeneous Markov process whose transition probabilities are
\[ p_{-,t}(x, B) = \begin{cases} 1 &\text{if } x = 0, B = \{0\}, \\ p_t(x, B) - p_t(x, -B) &\text{if } x > 0, B \subseteq (0, \infty), \\ 2p_t(x, (-\infty, 0]) &\text{if } x > 0, B = \{0 \}.  \end{cases} \]

\item (Ex 9.1.15, HW6) \textbf{Reflected Brownian motion} $\{ |W_t|, \calF_t \}$ is a homogeneous Markov process whose transition probabilities are $p_{+,t}(x,B) = p_t(x, B) + p_t(x, -B)$ for $x \geq 0$ and $B \subseteq [0,\infty)$.

\item (Ex 9.1.16, HW6) $Y_t = M_t - W_t$ is an $\calF_t$-Markov process with the same transition probabilities (and hence the same law) as reflected Brownian motion.

\end{itemize}

\section*{Section 9.2: Weak convergence and invariance principles}
\begin{itemize}
\item (Dfn 9.2.1) \textbf{Convergence in distribution:} $\{ X_n(t), t \geq 0\}$ of continuous sample functions \textbf{converge in distribution} to a stochastic process $\{X_\infty(t), t \geq 0 \}$ if the corresponding laws converge weakly in the topological space $C([0,\infty))$ with the topology of uniform convergence on compact subsets of $[0,\infty)$.

That is, if $g(X_n(\cdot)) \stackrel{d}{\goesto} g(X_\infty(\cdot))$ whenever $g: C([0,\infty)) \mapsto \bbR$ is Borel-measurable and is such that w.p. 1, the sample function of $X_\infty(\cdot)$ is not in the set $D_g$ of points of discontinuity of $g$ ($D_g = \{ X_\infty(\cdot) \in C([0,\infty)), \exists\; X_n(\cdot) \goesto X_\infty(\cdot) \text{ and } g(X_n) \not\goesto g(X_\infty) \}$).

\item (Thm 9.2.2) \textbf{Donsker's Invariance Principle:} If $\{ \xi_k\}$ are i.i.d. with $\bbE \xi_1 = 0$ and $\bbE \xi_1^2 = 1$, then for $S(\cdot)$ where $S(t) = \dis\sum_{k=1}^{\lf t \rf} \xi_k + (t - \lf t \rf)\xi_{\lf t \rf + 1}$, the stochastic processes $\widehat{S}_n(\cdot) = n^{-1/2}S(n\cdot)$ converges in distribution to the standard Wiener process.

\item (Prop 9.2.4) If the laws of $\{ X_n(\cdot)\}$ of continuous sample functions are uniformly tight in $C([0,\infty))$ and the f.d.d. of $\{ X_n(\cdot)\}$ converge weakly to the f.d.d. of $\{ X_\infty(\cdot)\}$, then $X_n(\cdot) \stackrel{d}{\goesto} X_\infty(\cdot)$.

\item (Thm 9.2.5) Arzel\`{a}-Ascoli Theorem.

\item (Eg 9.2.9, Ex 9.2.10, Eg 9.2.11) Examples of applying Donsker's invariance principle for different functionals.

\item (Cor 9.2.13) \textbf{Glivenko-Cantelli:} Suppose $\{ X_k, X\}$ i.i.d. and $x \mapsto F_X(x)$ is continuous. Then if $D_n = \dis\sup_{x \in \bbR} |F_X(x) - F_n(x)|$, as $n \goesto \infty$ we have $n^{1/2}D_n \stackrel{d}{\goesto} \dis\sup_{t \in [0,1]} |\widehat{B}_t|$, where $\widehat{B}_t$ is the standard Brownian bridge.

\item (Lem 9.2.16) Let $\{ W(t)\}$ be a standard Wiener process and let $k \mapsto T_{n,k}$ be non-decreasing, such that $T_{n, \lf nt \rf} \stackrel{P}{\goesto} t$ as $n \goesto \infty$, for each fixed $t \in [0,\ell]$. (Think of the $T_{n,k}$'s as sampling times.)

Then for the norm $\|x(\cdot)\| = \dis\sup_{t \in [0,\ell]} |x(t)|$ and $\widehat{S}_n(t) = S_n(nt)$, where $S_n(t) = W(T_{n, \lf t \rf}) + (t - \lf t \rf)[W(T_{n, \lf t \rf + 1}) - W(T_{n, \lf t \rf})]$, we have $\| \hat{S}_n - W \| \stackrel{P}{\goesto} 0$. In fact, $\hat{S}_n(\cdot) \stackrel{d}{\goesto} W(\cdot)$ in $C([0,\infty))$.

\item (Thm 9.2.19) \textbf{Skorohod's Representation:} Suppose $(W_t, \calF_t)$ is a Brownian Markov process such that $W_0 = 0$. Given the law $P_X$ of an integrable $X$ such that $\bbE X = 0$, there exists an a.s. finite $\calF_t$-stopping time $\tau$ such that $W_\tau \stackrel{d}{=} X$, $\bbE \tau = \bbE [X^2]$ and $\bbE [\tau^2] \leq 2 \bbE [X^4]$.

\item (Thm 9.2.20) \textbf{Strassen's Martingale Representation:} Suppose the probability space contains a discrete-time martingale $\{M_{\ell}, \calF_\ell\}$ such that $M_0 = 0$, as well as a standard Wiener process $\{W(t)\}$, independent of $\calF_\infty$ and $\{M_\ell\}$. Then
\begin{itemize}
\item The filtrations $\calF_{k,t} = \sg(\calF_k, \calF_t^W)$ are such that $(W(t), \calF_{k,t})$ is a Brownian Markov process for any $1 \leq k \leq \infty$.

\item There exist non-decreasing a.s. finite $\calF_{k,t}$-stopping times $\{T_k\}$, starting with $T_0 = 0$, where $\tau_k = T_k - T_{k-1}$ and the filtration $\calH_k = \calF_{k, T_k}$ are such that w.p. 1, $\bbE[\tau_k \mid \calH_{k-1}] = \bbE [D_k^2 \mid \calF_{k-1}]$ and $\bbE[\tau_k^2 \mid \calH_{k-1}] \leq 2\bbE [D_k^4 \mid \calF_{k-1}]$, where $D_k = M_k - M_{k-1}$ are the martingale differences, for all $k \geq 1$.
\item The discrete time process $\{ W(T_\ell)\}$ has the same f.d.d. as $\{ M_\ell\}$.
\end{itemize}

\item (Cor 9.2.21) \textbf{Skorohod's Representation for random walks:} Suppose $\xi_1$ integrable and of zero mean. The random walk $S_n = \sum_{k=1}^n \xi_k$ for i.i.d. $\{\xi_k\}$ can be represented as $S_n = W(T_n)$ for $T_0 = 0$, i.i.d. $\tau_k = T_k - T_{k-1} \geq 0$ such that $\bbE [\tau_1] = \bbE [\xi_1^2]$, and standard Wiener process $W$. Also, each $T_k$ is a stopping time for $\calF_t^W$.

\item (Thm 9.2.22) \textbf{Lindeberg's Martingale CLT:} Suppose that for any $n \geq 1$ fixed, $(M_{n,\ell}, \calF_{n, \ell})$ is a discrete-time $L^2$ martingale starting at 0, with martingale differences $D_{n,k} = M_{n,k} - M_{n, k-1}$ and predictable compensators $\langle M_n \rangle_\ell = \dis\sum_{k=1}^\ell \bbE [D_{n,k}^2 \mid \calF_{n, k-1}]$. If
\begin{enumerate}[label=(\alph*)]
\item For any fixed $t \in [0,1]$, $\langle M_n \rangle_{\lf nt \rf} \stackrel{P}{\goesto} t$ as $n \goesto \infty$, and
\item For each $\eps > 0$, $g_n(\eps) = \dis\sum_{k=1}^n \bbE [D_{n,k}^2 I \{ |D_{n,k}| \geq \eps\} \mid \calF_{n,k-1}] \stackrel{P}{\goesto} 0$ as $n \goesto \infty$,
\end{enumerate}
then as $n \goesto \infty$, the linearly interpolated, time-scaled $\widehat{S}_n(t) = M_{n,\lf nt \rf} + (nt - \lf nt \rf)D_{n, \lf nt \rf + 1}$ converges in distribution on $C([0,1])$ to the standard Wiener process $\{ W_t, t \in [0,1]\}$.

\item (Cor 9.2.23) \textbf{CLT for a single martingale:} Suppose $(M_\ell, \calF_\ell)$ is an $L^2$ martingale starting at 0. If
\begin{enumerate}[label=(\alph*)]
\item $\langle M \rangle_n / n \stackrel{P}{\goesto} 1$ as $n \goesto \infty$, and
\item For each $\eps > 0$, $\dfrac{1}{n}\dis\sum_{k=1}^n \bbE [(M_k - M_{k-1})^2; |M_k - M_{k-1}| \geq \eps\sqrt{n}] \goesto 0$ as $n \goesto \infty$,
\end{enumerate}
then as $n \goesto \infty$, the linearly interpolated, time-scaled $\widehat{S}_n(t) = n^{-1/2}[M_{\lf nt \rf} + (nt - \lf nt \rf)(M_{\lf nt \rf + 1} - M_{\lf nt \rf})]$ converges in distribution on $C([0,1])$ to the standard Wiener process $\{ W_t, t \in [0,1]\}$.

\item (Thm 9.2.27) \textbf{Kinchin's LIL:} Set $h(t) = \sqrt{2t \log \log (1/t)}$ for $t < 1/e$ and $\tilde{h}(t) = th(1/t)$. Then, for standard Wiener processes $W_t$ and $\widetilde{W}_t$, we have
\begin{align*}
\limsup_{t \downarrow 0} \frac{W_t}{h(t)} &= 1, &\liminf_{t \downarrow 0} \frac{W_t}{h(t)} &= -1, \\ 
\limsup_{t \goesto \infty} \frac{\widetilde{W}_t}{\tilde{h}(t)} &= 1, &\liminf_{t \goesto \infty} \frac{\widetilde{W}_t}{\tilde{h}(t)} &= -1.
\end{align*}

\item (Prop 9.2.9) \textbf{Hartman-Wintner's LIL for random walks:} Suppose $S_n = \sum_{k=1}^n \xi_k$, where $\xi_k$'s are i.i.d. with $\bbE \xi_1 = 0$ and $\bbE \xi_1^2 = 1$. Then w.p. 1, $\dis\limsup_{n \goesto \infty} \dfrac{S_n}{\sqrt{2n \log \log n}} = 1$.

\end{itemize}

\section*{Section 9.3: Brownian path: regularity, local maxima and level sets}
\begin{itemize}
\item (Dfn 9.3.1) For a continuous $f: [0, \infty) \mapsto \bbR$ and $\gamma \in (0,1]$, the \textbf{upper and lower (right) $\gamma$-derivatives} at $s \geq 0$ are the $\overline{\bbR}$-valued $D^\gamma f(s) = \dis\limsup_{u \downarrow 0} u^{-\gamma}[f(s+u) - f(s)]$ and $D_\gamma f(s) = \dis\liminf_{u \downarrow 0} u^{-\gamma}[f(s+u) - f(s)]$. (These always exist.)

The \textbf{Dini derivatives} correspond to the case of $\gamma = 1$. A continuous function is \textbf{differentiable from the right} at $s$ if $D^1 f(s) = D_1 f(s)$ is finite.

\item (Prop 9.3.2) \textbf{Nowhere differentiability of BM:} With probability 1, the sample function of a Wiener process is nowhere differentiable. More precisely, for $\gamma = 1$ and any $T \leq \infty$,
\[\bbP (\{ \om: -\infty < D_\gamma W_t(\om) \leq D^\gamma W_t(\om) < \infty \text{ for some } t \in [0,T] \}) = 0. \]

(Ex 9.3.3, HW8) The above actually holds for any $\gamma > 1/2$.

\item (Thm 9.3.5) \textbf{L\'{e}vy's modulus of continuity:} For $\dlt \in (0, 1]$, set $g(\dlt) = \sqrt{2\dlt \log (1/\dlt)}$. For a Wiener process $\{W_t, t\in [0,T] \}$ for $0 < T < \infty$, almost surely $\dis\limsup_{\dlt \downarrow 0} \dfrac{osc_{T, \dlt}(W_\cdot)}{g(\dlt)} = 1$, where $osc_{t, \dlt}(W_\cdot) = \dis\sup_{0 \leq h \leq \dlt}\sup_{0 \leq s \leq s+h \leq t} |W_{s+h} - W_s|$.

\item (Dfn) For non-random $b \in \bbR$, the \textbf{level set} of the standard Wiener process is defined by $\mathcal{Z}_\om(b) = \{t \geq 0: W_t(\om) = b \}$. The \textbf{zero set} is $\mathcal{Z}_\om = \mathcal{Z}_\om(0)$.

\item (Prop 9.3.7, Cor 9.3.8) Fix $b \in \bbR$. For almost every $\om \in \Om$, the level set $\mathcal{Z}_\om(b)$ of the standard Wiener process is closed, unbounded, of zero Lebesgue measure and having no isolated points.

\item (Remark 9.3.9) For almost every $\om$, the sample path of the Wiener process is monotone in no interval.

\item (Dfn 9.3.11) Suppose $f: [0,\infty) \mapsto \bbR$. $t \geq 0$ is a \textbf{point of local maximum} if there is a neighborhood such that $f(t) \geq f(s)$ for $s$ in that neighborhood. It is a \textbf{strict point of local maximum} if $f(t) > f(s)$ for any point in a neighborhood. It is a \textbf{point of increase} if there exists $\dlt > 0$ s.t. $f((t-h)_+) \leq f(t) \leq f(t+h)$ for all $h \in (0, \dlt]$.

\item (Prop 9.3.13) For almost every $\om \in \Om$, the set of points of local maximum for the Wiener sample path is a countable dense subset of $[0,\infty)$, and all local maxima are strict.

\item (Thm 9.3.14) Almost every sample path of the Wiener process has no point of increase (or decrease).

\end{itemize}

\section*{Other Stuff}
\subsection*{Brownian Motion Facts}
\begin{itemize}
\item (Dfn 7.3.12) $\{W_t, t \geq 0\}$ is called a \textbf{Brownian motion/Wiener process} starting at $x \in \bbR$ if it is a Gaussian process with mean function $m(t) = x$, auto-covariance $\text{Cov}(W_t, W_s) = t \wedge s$, and its sample functions are continuous.

If $x=0$, it is called \textbf{standard Brownian motion}.

\item (Ex 7.3.13c, HW2) For any finite $T$, $\{ W_t, t \in [0,T]\}$ can be viewed as the random variable $W_\cdot: (\Om, \calF) \mapsto (C([0,T]), \|\cdot\|_\infty)$, which is measurable w.r.t. the Borel $\sg$-algebra on $C([0,T])$, and is a.s. locally $\gamma$-H\"{o}lder continuous for any $\gamma < 1/2$.

Kinchin's LIL tells us that Brown motion sample paths are not $\gamma$-H\"{o}lder continuous for any $\gamma \geq 1/2$.

\item (Ex 7.3.13d, HW2) Brownian motion is non-stationary but has stationary, independent increments.

\item (Ex 7.3.16a, HW2) For $s < t$, $W_s \mid W_t \sim \calN \left( \displaystyle\frac{s}{t} W_t, \displaystyle\frac{s(t-s)}{t} \right)$. (Proof: Look at joint density function of $W_s$ and $W_t$.)

\item (Ex 7.3.16b, HW2) $\displaystyle\lim_{t \goesto \infty} \frac{W_t}{t} = 0$ (a.s. limit).

\item (Ex 8.2.35d, HW4) $\dis\limsup_{t \goesto \infty} W_t = \infty$ w.p. 1 and $\dis\liminf_{t \goesto -\infty} W_t = -\infty$ w.p. 1.

\item (Ex 8.2.7, HW3) Important collection of martingales based on Brownian motion and $u_0(t, y, \t) = \exp (\t y - \t^2 t /2)$.
\begin{itemize}
\item For any $\t \in \bbR$, $\{ u_0(t, B_t, \t)\} = \{ \exp (\t B_t - \t^2 t / 2) \}$ is a martingale.

\item Let $u_{k+1}(t,y,\t) = \dfrac{\partial}{\partial \t} u_k(t, y, \t)$. For any $k \geq 0$, $u_k(t, B_t, \t)$ is a martingale.

\item Evaluating at $\t = 0$, we get that $B_t^2 - t$, $B_t^3 - 3tB_t$, $B_t^4 - 6tB_t^2 + 3t^2$ and $B_t^6 - 15tB_t^4 + 45t^2B_t^2 - 15t^3$ are all martingales.
\end{itemize}

\item $(x + B_t)^2 - (t+x^2)$ is a martingale.

\item (Ex 8.2.36, HW4) \textbf{Exit times:} For $a,b > 0$, let $\tau_{a,b} = \inf \{t \geq 0: W_t \notin (-a, b) \}$.
\begin{itemize}
\item $\tau_{a,b}$ is an a.s. finite $\calF_t^W$-stopping time, and $\bbP \left(W_{\tau_{a,b}} = -a \right) = \dfrac{b}{a+b}$.

\item For all $s \geq 0$, $\bbE [e^{-s \tau_{a,b}}] = \dfrac{\sinh (a\sqrt{2s}) + \sinh(b\sqrt{2s})}{\sinh [(a+b)\sqrt{2s}]}$.

\item $\bbE \tau_{a,b} = ab$ and $\var \tau_{a,b} = \dfrac{ab}{3}(a^2 + b^2)$.
\end{itemize}

\item (Eg 8.2.51) \textbf{Quadratic variation:} A quadratic variation for the standard Brownian motion is $\langle W \rangle_t = t$. By Ex 8.2.46, this implies that the total variation of the Brownian sample path is a.s. infinite on any interval of positive length.

\item (Eg 8.3.6) Brownian motion is a homogeneous Markov process with semi-group $p_t(x, B) = \dis\int_B \dfrac{e^{-(y-x)^2/2t}}{\sqrt{2\pi t}} dy$.

\item (Ex 8.3.20, HW5) Any Brownian Markov process is a strong Markov process.

\item \textbf{Wald's Lemma for Brownian motion:} If $\{W_t\}$ is a standard Brownian motion and $\tau$ is a Markov time s.t. $\bbE \tau < \infty$, then $\bbE [B_\tau] = 0$ and $\bbE [B_\tau^2] = \bbE \tau$. If $\{B_{t \wedge \tau} \}$ is dominated by an integrable random variable, then we still have $\bbE [B_\tau] = 0$.

\item (Karatzas \& Shreve Prop 2.8.15, p100) Define the \textbf{last exit time} $\t_t = \sup \{ 0 \leq s \leq t: W_s = M_t \}$. Then for $a \in \bbR$, $b \geq a^+$, $0 < s < t$, we have
\[ \bbP (W_t \in da, M_t \in db, \t_t \in ds) = \frac{b(b-a)}{\pi \sqrt{s^3(t-s)^3}}\exp \left[-\frac{b^2}{2s} - \frac{(b-a)^2}{2(t-s)} \right] da\; db\; ds. \]

\item (M\"{o}rters \& Peres Thm 2.37) Let $W_1(t)$ and $W_2(t)$ be independent Brownian motions. Let $T_a = \inf \{ t: W_1(t) = a\}$. Then $W_2(T_a)$ has density $\dfrac{a}{\pi(a^2 + x^2)}$.

\end{itemize}

\subsection*{$k$-dimensional Brownian Motion Facts}
\begin{itemize}
\item (Ex 8.2.38, HW4) Let $R_t = \| \underline{W}(t) \|_2$ be the Euclidean distance of the process from the origin. Let $\t_b = \inf\{ t \geq 0: R_t \geq b\}$ be the first hitting time of a sphere of radius $b > 0$ centered at the origin. Then $M_t = R_t^2 - kt$ is an $\calF_t^W$-martingale of continuous sample functions, and $\t_b$ is an a.s. finite $\calF_t^W$ stopping time with $\bbE [\t_b] = b^2/k$.
\end{itemize}

\subsection*{Brownian Motion with Drift Facts}
\begin{itemize}
\item We write it as $Z_t^{(r, \sg)} = \sg W_t + rt + x$, with non-random drift $r \in \bbR$ and diffusion coefficient $\sg > 0$.

\item (Ex 8.3.10, HW5) $Z_t^{(r, \sg)}$ is a homogeneous Markov process.

\item Since $\exp \left[ \t W_t - \dfrac{\t^2 t}{2} \right]$ is a martingale for all $\t$, so is $\exp \left[ \t (W_t + rt) - \dfrac{(\t^2 + 2r\t)t}{2} \right]$.

\item \textbf{Hitting times:} Let $Z_t^{(r)} = W_t + rt$. For $b > 0$, let $\tau_b^{(r)} = \inf \{ t \geq 0: Z_t^{(r)} \geq b\}$.
\begin{itemize}
\item (Ex 8.2.35, HW4) $\tau_b^{(r)}$ is an $\calF_t^W$-stopping time.
\item (Karatzas \& Shreve p197) $\tau_b^{(r)}$ has density
\[ \bbP \left(\tau_b^{(r)} \in dt \right) = \frac{b}{\sqrt{2\pi t^3}} \exp \left[ - \frac{(b+rt)^2}{2t} \right] dt. \]
\item (Karatzas \& Shreve p197) $\bbP \left( \tau_b^{(r)} < \infty \right) = \exp [-rb - |rb|]$.
\end{itemize}

\textbf{Case 1: $r > 0$.}
\begin{itemize}
\item $\bbP \left( \tau_b^{(r)} < \infty \right) = 1$.
\item $\bbE [\tau_b^{(r)}] < \infty$.
\end{itemize}

\textbf{Case 2: $r = 0$.}
\begin{itemize}
\item (Ex 8.2.35, HW4) $\bbE \left[\exp \left( -s\tau_b^{(0)} \right) \right] = \exp \left[-b\sqrt{2s} \right]$.
\item (Ex 8.2.35, HW4) $\bbP \left( \tau_b^{(0)} < \infty \right) = 1$.
\item (Prob Qual 2011-6, Session 11)$\bbE [\tau_b^{(0)}] = \infty$.
\end{itemize}

\textbf{Case 3: $r < 0$.}
\begin{itemize}
\item (Ex 8.2.35, HW4) $\bbE \left[\exp \left( -s\tau_b^{(r)} \right) \right] = \exp \left[-b(\sqrt{r^2 + 2s} -r) \right]$.
\item (Ex 8.2.35, HW4) $\bbP \left( \tau_b^{(r)} < \infty \right) = e^{2rb} < 1$. Hence, $\bbE [\tau_b^{(r)}] = \infty$.
\end{itemize}

\item (Ex 8.2.36, HW4) \textbf{Exit times from $(-a,b)$:} Now consider $Z_t^{(r)}$ for all $r \in \bbR$. For $a,b > 0$, let $\tau_{a,b}^{(r)} = \inf \{t \geq 0: Z_t^{(r)} \notin (-a, b) \}$. Then $\tau_{a,b}^{(r)}$ is an a.s. finite $\calF_t^W$-stopping time, and for $r \neq 0$,
\[ \bbP \left(Z_{\tau_{a,b}^{(r)}}^{(r)}) = -a \right) = 1 - \bbP \left(Z_{\tau_{a,b}^{(r)}}^{(r)}) = b \right) = \frac{1 - \exp(-2rb)}{\exp(2ra) - \exp(-2rb)}. \]
When $r = 0$, we have $\bbP \left(W_{\tau_{a,b}^{(0)}} = -a \right) = \dfrac{b}{a+b}$.

\item (Eg 8.3.6) $\{Z_t^{(r)}\}$ is a homogeneous Markov process with semi-group $p_t(x+rt, B)$, where $p_t(x, B) = \dis\int_B \dfrac{e^{-(y-x)^2/2t}}{\sqrt{2\pi t}} dy$.



\end{itemize}

\subsection*{Brownian Bridge Facts}
\begin{itemize}
\item The standard Brownian bridge is a Gaussian process with mean function 0 and auto-covariance function $c(s, t) = s \wedge t - st$.

\item (Ex 7.3.15a) We can write the standard Brownian bridge as $\widehat{B}_t = W_t - \min (t, 1) W_1$.

\item (Ex 7.3.16c,d, HW2) We can also write it as $\widetilde{B}_t = (1-t)W_{t/(1-t)}$ with $\widetilde{B}_0 = 0$, or $W_t \mid W_1 = 0$, $t \in [0,1]$.

\item (Ex 8.3.10, HW5) Brownian bridge is a Markov process with stationary increments, but is not a homogeneous Markov process.

\item \textbf{Useful transformation for stopping times:} By time inversion, we have $\left\{ \dis\sup_{t \in [0,1]} \{ W_t - tW_1 \} \geq b \right\} = \left\{ \dis\sup_{s \geq 1} \{ \widetilde{W}_s - \widetilde{W}_1 - sb \} \geq 0 \right\}$, where $\widetilde{W}_t = t W_{1/t}$.

\item (Ex 9.2.14, HW7) $\bbP \left( \dis\sup_{t \in [0,1]} \widehat{B}_t \geq b \right) = \exp (-2b^2)$.

For any non-random $a,c > 0$, $\bbP \left( \dis\inf_{t \in [0,1]}\widehat{B}_t \leq -a \text{ or } \dis\sup_{t \in [0,1]}\widehat{B}_t \geq c \right) = \dis\sum_{n \geq 1} (-1)^{n-1} (p_n + r_n)$, where $p_{2n} = r_{2n} = \exp [-2(na+nc)^2]$, $r_{2n+1} = \exp [-2(na+nc+c)^2]$ and $p_{2n+1} = \exp [-2(na+nc+a)^2]$.

\item (Ex 9.2.14, HW7) Let $F_{KS}(\cdot)$ be the distribution function for $\dis\sup_{t \in [0,1]} |\widehat{B}_t|$. Then $F_{KS}(b) = 1 - 2 \dis\sum_{n=1}^\infty (-1)^{n-1}e^{-2n^2 b^2}$.

\item (Lemma used in HW7 Ex 9.2.14) For $b > 0$, let $P_{b, \eps} = \bbP (W_t \geq b \text{ for some } t \in [0,1] \mid |W_1| < \eps)$. Then $P_{b,\eps} = \dfrac{\bbP(|W_1 - 2b| < \eps)}{\bbP(|W_1| < \eps)}$, and $\dis\lim_{\eps \downarrow 0} P_{b, \eps} = \exp(-2b^2)$.

\end{itemize}

\subsection*{Fractional Brownian Motion Facts}
\begin{itemize}
\item (Ex 7.3.17, HW2) For $H \in (0,1)$, fractional Brownian motion of Hurst parameter $H$ is the centered Gaussian stochastic process with auto-covariance function $c(s, t) = \dfrac{1}{2}\left[ |t|^{2H} + |s|^{2H} - |t-s|^{2H} \right]$.

\item (Ex 7.3.17b) Fractional Brownian motion exists and has a continuous modification that is locally $\gamma$-H\"{o}lder continuous for any $0 < \gamma < H$.

\item (Ex 7.3.17c) When $H = 1/2$, fBM is the standard Wiener process.

\item (Ex 7.3.17d) For any non-random $b > 0$, $\{ b^{-H}X_{bt}, t \geq 0\}$ is also an fBM with Hurst parameter $H$.

\item (Ex 7.3.17e) The increments of fBM are stationary for all values of $H$. The increments of fBM are independent only when $H = 1/2$.

\end{itemize}

\subsection*{Geometric Brownian Motion Facts}
\begin{itemize}
\item (Ex 8.3.10) This is defined by $Y_t = e^{W_t}$.

\item (Ex 8.3.10, HW5) Geometric Brownian motion is a homogeneous Markov process whose increments are neither independent nor stationary.
\end{itemize}

\subsection*{Ornstein-Uhlenbeck Process Facts}
\begin{itemize}
\item (Ex 7.3.15b) The Ornstein-Uhlenbeck process is given by $U_t = e^{-t/2}W_{e^t}$.

\item The OU process is a stationary process.

\item (Ex 8.3.10, HW5) The OU process is a homogeneous Markov process.

\item (Qual 2010 Qn5) The OU is a homogeneous, zero-mean, Gaussian Markov process such that $\bbE [U_t \mid U_0] = e^{-t/2} U_0$ and $\var (U_t \mid U_0) = 1 - e^{-t}$.

\item (Qual 2010 Qn5) $\bbE [U_t] = 0$, $\text{Cov}(U_t, U_s) = \exp \left( - \dfrac{|s-t|}{2} \right)$.

\item (Qual 2010 Qn5) The transition kernel is $p_t(x,y) = \dfrac{1}{\sqrt{2\pi(1 - e^{-t})}} \exp \left[ - \dfrac{1}{2(1-e^{-t})}(y - e^{-t/2}x)^2 \right]$.
\end{itemize}

\subsection*{Poisson process}
\begin{itemize}
\item (Dfn 3.4.8) The Poisson process of rate $\lmb > 0$, denoted $N_t$, is the unique counting process with gaps between jump times being i.i.d. $\text{Exp}(\lmb)$ variables.

\item The $k^{th}$ arrival time $T_k$ has distribution $\text{Gam}(k, \lmb)$ (shape-rate).

\item As $t \goesto \infty$, $N_t/t \stackrel{a.s.}{\goesto} \lmb$.

\item (Prop 3.4.9) The Poisson process has independent increments, and $N_t - N_s \sim \text{Pois}(\lmb(t-s))$.

\item (Eg 8.2.5) The \textbf{compensated Poisson process} $M_t = N_t - \lmb t$ is a martingale.

\item (Eg 8.2.53) $M_t^2 - \lmb t$ is a right-continuous martingale, so $\langle M \rangle_t = \lmb t$.

\item (Eg 8.3.6) The \textbf{Poisson process with drift} is $N_t^{(r)} = N_t + rt + x$. This is a homogeneous Markov process with semi-group $q_t(x + rt, B)$, where $q_t(x, B) = e^{-\lmb t} \dis\sum_{k=0}^\infty \dfrac{(\lmb t)^k}{k!} I_B(x+k)$.

\item (Ex 8.3.20, HW5) The Poisson process is a strong Markov process.

\end{itemize}

\end{document}