\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,amssymb,graphicx,mathtools,flexisym, float, enumitem}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\arabic{page}}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\thefigure}{\arabic{figure}}
\renewcommand{\thetable}{\arabic{table}}

%
% Macros for shortcuts
%
\newcommand{\dis}{\displaystyle}
\def\lc{\left\lceil}   
\def\rc{\right\rceil}
\def\lf{\left\lfloor}   
\def\rf{\right\rfloor}
\newcommand\bbC{\mathbb{C}}
\newcommand\bbE{\mathbb{E}}
\newcommand\bbN{\mathbb{N}}
\newcommand\bbP{\mathbb{P}}
\newcommand\bbQ{\mathbb{Q}}
\newcommand\bbR{\mathbb{R}}
\newcommand\bbZ{\mathbb{Z}}
\newcommand\calA{\mathcal{A}}
\newcommand\calB{\mathcal{B}}
\newcommand\calC{\mathcal{C}}
\newcommand\calF{\mathcal{F}}
\newcommand\calG{\mathcal{G}}
\newcommand\calH{\mathcal{H}}
\newcommand\calL{\mathcal{L}}
\newcommand\calM{\mathcal{M}}
\newcommand\calN{\mathcal{N}}
\newcommand\calP{\mathcal{P}}
\newcommand\calR{\mathcal{R}}
\newcommand\calS{\mathcal{S}}
\newcommand\calU{\mathcal{U}}
\newcommand\calX{\mathcal{X}}
\newcommand\calY{\mathcal{Y}}
\newcommand\dlt{\delta}
\newcommand\Dlt{\Delta}
\def\eps{\varepsilon}
\newcommand\lmb{\lambda}
\newcommand\Lmb{\Lambda}
\newcommand\om{\omega}
\newcommand\Om{\Omega}
\newcommand\sg{\sigma}
\newcommand\Sg{\Sigma}
\def\t{\theta}
\newcommand\T{\Theta}
\newcommand\vp{\varphi}
\newcommand\equivto{\Leftrightarrow}
\newcommand\goesto{\rightarrow}
\newcommand\var{\text{Var }}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

\begin{document}
\title{STATS 310A Notes}
\author{Kenneth Tay}
\date{\vspace{-3ex}}
\maketitle

% Basic measure theory
\section*{Basic measure theory (Lec 1-6)}
\begin{itemize}
\item (Billingsley Prob 2.4, HW1) The union of $\sg$-fields need not be a $\sg$-field. The countable union of fields is a field.

\item (Billingsley Prob 2.5) Let $\calA$ be a collection of subsets of $\Om$. The field generated by $\calA$ is equal to the collection of sets of the form $\dis\bigcup_{i=1}^m\bigcap_{j=1}^{n_i} A_{ij}$, where either $A_{ij} \in \calA$ or $A_{ij}^c \in \calA$, and where the $m$ sets $\bigcap_{j=1}^{n_i} A_{ij}$ are disjoint.

\item \textbf{Definition of outer measure}: Let $P$ be a probability on field $\calF_0$. For every $A \subseteq \Om$, define $P^*(A) := \inf \displaystyle\sum_{i=1}^\infty P(B_i)$, where $A \subseteq \displaystyle\bigcup B_i$, $B_i \in \calF_0$. (Covers can be countably infinite in size.) Note that $P^*$ is countably sub-additive.

\item A probability measure on a field has a unique extension to the generated $\sg$-field.

\item \textbf{$\pi-\lmb$ Theorem}: If $\calP$ is a $\pi$-system and $\calL$ is a $\lmb$-system and $\calP \subseteq \calL$, then $\sg(\calP) \subseteq \calL$.

\item \textbf{Law of the iterated logarithm}: Let $\{ Y_n \}$ be iid random variables with zero mean and variance 1. Then almost surely, $\limsup \displaystyle\frac{Y_1 + \dots + Y_n}{\sqrt{2n \log \log n}} = 1$.

\item For the simple symmetric ($\pm 1$) random walk, let $M_n = \max (S_1, \dots, S_n)$. Then for every integer $c \geq 1$, $P (M_n \geq c) \leq P (S_n \geq c) + P (S_n > c) \leq 2 P (S_n \geq c)$.

\item For the simple symmetric ($\pm 1$) random walk, $P\left(\frac{S_n}{n} \geq \eps \right) \leq 2 \exp \left( - \frac{n\eps^2}{2} \right)$. (Proof in HW2 Q5.)

\item \textbf{Definition of a general measure on a field}: $\mu: \calF \mapsto \bbR$ is a measure if $\mu (\emptyset) = 0$, $\mu$ is non-negative and countably additive.

\item If $\mu_1$ and $\mu_2$ are measures on $\sg(\calP)$ such that they agree on $\pi$-system $\calP$ and are $\sg$-finite on $\calP$, then they agree on $\sg(\calP)$.

\item \textbf{Definition of outer measure}: A set function $\mu^*$ defined on all subsets of $\Om$ is an outer measure if it is non-negative, monotone, countably sub-additive and $\mu^*(\emptyset) = 0$.

\item If $\mu^*$ is an outer measure on subsets of $\Om$, then $\mu^*$ is a measure on $\calM^*(\mu^*) = \{ A \subseteq \Om: \forall E \subseteq \Om, \mu^*(E) = \mu^*(A \cap E) + \mu^*(A^c \cap E) \}$.

\item \textbf{Definition of semi-ring}: A collection of subsets $\calR$ of $\Om$ is a semi-ring if it contains $\emptyset$, is closed under finite intersections, and if $A, B \in \calR$ with $A \subseteq B$, then $B \setminus A = \displaystyle\bigcup_{i=1}^n C_i$ with $C_i \in \bbR$, $C_i$'s disjoint.

Examples of semi-rings: finite subsets of $[0,1]$, $\infty$ rectangles in $\bbR^d$ (must extend to $\infty$ in at least 1 direction), and finite rectangles in $\bbR^d$.

\item \textbf{Extension theorem}: If $\mu: \calR \mapsto [0, \infty]$ such that $\mu$ is countably sub-additive, finitely additive on $\calR$ and $\mu(\emptyset) = 0$, then $\mu$ has an extension to a measure on $\sg(\calR)$. If $\mu$ is $\sg$-finite, this measure is unique.

\end{itemize}

% Distribution functions & random variables
\section*{Distribution functions, random variables, integration (Lec 7-10, 12)}
\begin{itemize}
\item Let $F: \bbR^k \mapsto \bbR$ be monotonically increasing, right-continuous, such that $F(\infty) = 1$ and $F(-\infty) = 0$. If $\Dlt_A (F) \geq 0$ for all finite rectangles $A$, then there exists a unique probability measure $\mu$ on Borel sets of $\bbR^k$ such that $\mu(A) = \Dlt_A(F)$ for all $A$.

\item (Billingsley Prob 14.8, HW4) If a distribution function $F$ is everywhere continuous, then it is uniformly continuous.

\item If $X$ has CDF $F$, then $F(X)$ is a $\text{Unif}(0,1)$ variable.

\item Useful proposition to manipulate inverses: Let $T$ be a measurable map from $(\Om, \calF)$ to $(\Om', \calF')$. For a collection of sets $\{B_i'\}_{i \in I}$ in $\calF'$,
\begin{enumerate}
\item $[T^{-1}(B')]^c = T^{-1}(B^{\prime c})$.
\item $T^{-1}\left(\displaystyle\bigcup_{i \in I} B_i'\right) = \displaystyle\bigcup_{i \in I} T^{-1}(B_i')$.
\item $T^{-1}\left(\displaystyle\bigcap_{i \in I} B_i'\right) = \displaystyle\bigcap_{i \in I} T^{-1}(B_i')$.
\end{enumerate}

\item \textbf{Definition of push-forward:} Suppose we have $(\Om, \calF, \mu)$ a measure space, $(\Om', \calF')$ a measurable space, $T: (\Om, \calF) \rightarrow (\Om', \calF')$ measurable. The push-forward $\mu^{T^{-1}}$, a measure on $(\Om', \calF')$, is defined by 
\begin{equation*} \mu^{T^{-1}}(B') := \mu(T^{-1}(B')) \qquad \text{ for } B' \in \calF'. \end{equation*}

\item \textbf{Definition of simple function:} A measurable function which takes on finitely many values, i.e. exist $x_i \in \bbR \cup \pm \infty$ and a partition of $\Om$, $A_i \in \calF$, such that $f(\om) = \dis\sum_{i=1}^n x_i \dlt_{A_i}(\om)$.

\item For every non-negative measurable function $f$, there exists a non-decreasing sequence of simple functions $f_n$ such that $f_n(\om) \rightarrow f(\om)$ for all $\om$.

\item \textbf{Definition of integral:} For $f \geq 0$, define $\dis\int f d\mu := \sup \dis\sum_{i=1}^n \nu_i \mu(A_i)$, where the sup is taken over all \textbf{finite} measurable partitions $\{A_i\}$ of $\Om$, and $\nu_i = \inf_{\om \in A_i} f(\om)$.

\item \textbf{Monotone Convergence Theorem:} For non-negative $f$, if $f_n(\om) \nearrow f(\om)$ for all $\om$, then $\dis\lim_{n \rightarrow \infty} \int f_n d\mu = \int f d \mu = \int \lim_{n \rightarrow \infty} f_n d\mu$.

\item \textbf{Fatou's Lemma:} Let $\{ f_n \}$ be any sequence of non-negative measurable functions. Then $\dis\int \liminf f_n d\mu \leq \liminf \int f_n d\mu$.

\item \textbf{Dominated Convergence Theorem:} Let $\{ f_n \}$ be a sequence of measurable functions such that $f_n \rightarrow f$ a.s. and there exists an integrable function $g \geq 0$ such that $| f_n | \leq g$ a.s. Then $f$ is measurable and integrable, and $\dis\lim_{n \rightarrow \infty} \int f_n d\mu = \int f d\mu$.

\item For $X \geq 0$, $\bbE X = \dis\int_0^\infty P(X \geq t) dt = \dis\int_0^\infty P(X > t)dt$.

\item Let $X$ have MGF $M$. If $M(s)$ is finite on some non-empty interval $(-s_0, s_0)$, then $M$ is infinitely differentiable and $M^{(k)}(0) = \bbE [X^k]$.

\end{itemize}

% Product spaces
\section*{Product spaces (Lec 11)}
Let $(X, \calX)$, $(Y, \calY)$ be two measurable spaces.

\begin{itemize}
\item \textbf{Projection mappings} are $\pi_x : (x, y) \mapsto x$ and $\pi_y: (x, y) \mapsto y$.

\item \textbf{Product $\sg$-algebra}: $\calX \times \calY := \sg(\pi_x, \pi_y)$ (i.e. the $\sg$-algebra generated by the projection mappings).

\item \textbf{Cylinder sets}: $\calC = \{ \pi_x^{-1}(A) \cup \pi_y^{-1}(B) : A \in \calX, B \in \calY \}$.

\item \textbf{Measurable rectangles}: $\calP = \{ A \times B: A \in \calX, B \in \calY \}$.

\item $\calX \times \calY = \sg(\calC) = \sg(\calP) = \sg(\{\text{all finite disjoint unions of measurable rectangles} \})$.

\item \textbf{Sections:} For $A \subseteq Z$, sections are defined to be $A_x = \{ y: (x, y) \in A \}$, $A_y = \{ x: (x, y) \in A \}$.

\item \textbf{Sectioning lemma:} If $f: (Z, \mathcal{Z}) \rightarrow (W, \mathcal{W})$ is measurable, then $f_x : Y \rightarrow W$ is measurable. 

If $A \in \mathcal{Z}$, then $A_x$, $A_y$ are measurable.

\item \textbf{Definition of Markov kernel:} A \textbf{Markov kernel} is a function $K(x, B): (X, \mathcal{Y}) \rightarrow [0,1]$ such that
\begin{enumerate}
\item For every fixed $x$, $K(x, \cdot)$ is a probability on $(Y, \mathcal{Y})$, and
\item For every fixed $B$, the map $x \mapsto K(x, B)$ is measurable.
\end{enumerate}

\item Let $\mu$ be a probability on $(X, \mathcal{X})$. We can define a probability $\mu \times K$ on $(Z, \mathcal{Z})$ by $\mu \times K (A) := \dis\int K(x, A_x) \mu(dx)$.

\item \textbf{Fubini's theorem:} If $f: Z \rightarrow \bbR$ is Borel-measurable and non-negative, then 
\begin{enumerate}
\item $x \mapsto \dis\int f(x, y) K(x, dy)$ is $\mathcal{X}$-measurable, and
\item \begin{equation*}\int f d(\mu \times K) = \int_X \left[\int_Y f(x, y) K (x, dy) \right] \mu(dx).\end{equation*}
\end{enumerate}

\item (Durrett Thm 1.7.2 p37) \textbf{Durrett's version of Fubini:} If $f \geq 0$ or $\dis\int |f| d\mu < \infty$ (here $\mu = \mu_1 \times \mu_2$, then
\begin{equation*}
\int_X \int_Y f(x,y) \mu_2(dy) \mu_1(dx) = \int_{X \times Y} f d\mu = \int_Y \int_X f(x,y) \mu_1 (dx) \mu_2 (dy).
\end{equation*}


\end{itemize}

% Strong law of large numbers
\section*{Convergence, strong \& weak law of large numbers (Lec 13)}
\begin{itemize}
\item (Billingsley Prob 20.24, HW7) In a discrete probability space, convergence in probability is equivalent to a.s. convergence.

\item (Durrett Thm 2.4.1 p73) \textbf{Strong law of large numbers:} Let $X_1, X_2, \dots$ be pairwise independent identically distributed random variables with $\bbE |X_i| < \infty$. Let $\bbE X_i = \mu$ and $S_n = X_1 + \dots + X_n$. Then as $n \goesto \infty$, $S_n / n \goesto \mu$ a.s.

\item (Durrett Them 2.4.5 p75) Let $X_1, X_2, \dots$ be i.i.d. with $\bbE X_i^+ = \infty$ and $\bbE X^- < \infty$. Then $S_n/n \goesto \infty$ a.s.

\item \textbf{SLLN without i.i.d. assumption:} Let $X_1, X_2, \dots$ be mutually independent such that the $X_k$'s each have finite variance and $\dis\sum_{k=1}^\infty\frac{\var X_k}{k^2} < \infty$. Then $\bar{X}_n - \bbE [\bar{X}_n] \stackrel{a.s.}{\goesto} 0$.

\item \textbf{Siegmund's theorem about deviations}: Let $X_i$ be i.i.d. with mean 0 and variance 1. For each $\eps > 0$, let $m_\eps = \sup \{ n \geq 0: |S_n / n| \geq \eps \}$. Then for $0 \leq x < \infty$, $\bbP (\eps^2 m_\eps \leq x) \goesto 2\Phi(x) - 1$ as $\eps \goesto 0$.

\item (Durrett Thm 2.2.6) \textbf{Weak law for triangular arrays:} For each $n$, let $X_{n,k}$, $1 \leq k \leq n$ be independent. Let $b_n > 0$ with $b_n \goesto \infty$, and let $\bar{X}_{n,k} = X_{n,k} 1_{\{ |X_{n,k} \leq b_n|\}}$. Suppose that as $n \goesto \infty$,
\begin{enumerate}
\item $\dis\sum_{k=1}^n P(|X_{n,k} > b_n|) \goesto 0$, and
\item $\dis\frac{1}{b_n^2}\sum_{k=1}^n \bbE \bar{X}_{n,k}^2 \goesto 0$.
\end{enumerate}
If we let $S_n = X_{n,1} + \dots + X_{n,n}$ and $a_n = \dis\sum_{k=1}^n \bbE \bar{X}_{n,k}$, then $\dis\frac{S_n - a_n}{b_n} \goesto 0$ in probability.

\item (Durrett Thm 2.2.7) \textbf{Weak law of large numbers:} Let $X_1, X_2, \dots$ be i.i.d. with $x P (|X_i| > x) \goesto 0$ as $x \goesto \infty$. If we let $S_n = X_1 + \dots + X_n$ and $\mu_n = \bbE \left[ X_1 1_{\{ |X_1 \leq n|\}} \right]$, then $S_n/n - \mu_n \goesto 0$ in probability.

\item \textbf{Kolmogorov Three Series Theorem:} Let $X_1, X_2, \dots$ be independent random variables. The random series $\dis\sum_{n=1}^\infty X_n$ converges almost surely in $\bbR$ iff the following conditions hold for some $A > 0$:
\begin{enumerate}
\item $\dis\sum_{n=1}^\infty \bbP(|X_n| \geq A)$ converges,
\item Let $Y_n = X_n 1_{\{|X_n| \leq A \}}$, then $\dis\sum_{n=1}^\infty \bbE Y_n$ converges, and
\item $\dis\sum_{n=1}^\infty \var Y_n$ converges.
\end{enumerate}

\item (Durrett Thm 2.5.3) \textbf{Special case of three series theorem:} In the set-up above, further assume that $\bbE X_n = 0$ for all $n$. If $\dis\sum_{n=1}^\infty \var X_n < \infty$, then $\dis\sum_{n=1}^\infty X_n$ converges almost surely.

\item (Billingsley Prob 22.3, HW7) \textbf{Generalized Borel-Cantelli lemmas:} Suppose $\{X_n\}$ are non-negative. If $\sum \bbE X_n < \infty$, then $\sum X_n$ converges w.p. 1. If the $X_n$ are independent and uniformly bounded, and $\sum \bbE X_n = \infty$, then $\sum X_n$ diverges w.p. 1.

\end{itemize}

% Stein's method
\section*{Stein's method (Lec 14-15)}
\begin{itemize}
\item Set-up:
\begin{itemize}
\item $\{ X_i \}_{i \in I}$ a collection of 0/1-valued random variables ($I$ some index set),
\item $W := \displaystyle\sum_{i \in I} X_i$,
\item $p_i := \bbE X_i = P\{X_i = 1 \}$,
\item $\lmb := \sum_{i \in I} p_i = \bbE W$.
\item A \textbf{graph} is an ordered pair $(I, E)$, where $I$ is the set of vertices and $E \subseteq I \times I$ is the set of edges. $E$ must be symmetric (i.e. $(i, j) \in E \Leftrightarrow (j, i) \in E$) and has no loops (i.e. $(i, i) \notin E$ for all $i$).

\item A graph $(I, E)$ is a \textbf{dependency graph} for $\{ X_i \}_{i \in I}$ if for any two disjoint subsets $I_1, I_2 \subseteq I$ with no edges between them, $\{ X_i \}_{i \in I_1}$, $\{ X_j \}_{j \in I_2}$ are independent. (A dependency graph need not be unique.)

\item For a vertex $i \in I$, the \textbf{neighborhood} of $i$ is $N_i := \{ i \} \cup \{j : (i, j) \in E \}$.
\end{itemize}

\item Let $P_\lmb(A) :=$ probability that value of $\text{Poisson}(\lmb)$ random variable falls in $A$. For every $A \subseteq \bbN$, there is a unique function $f: \bbN \rightarrow \bbR$ such that 
\begin{equation*} \lmb f(w + 1) - wf(w) = \dlt_A(w) - P_\lmb(A) \end{equation*}
for $w = 0, 1, 2, \dots$. Moreover, $|f(w)| \leq 1.25$ and $|f(w+1) - f(w)| \leq \min(3, \lmb^{-1})$.

\item \textbf{Stein's Equation}: $Z \sim \text{Poisson}(\lmb)$ if and only if for every $f : \bbN \rightarrow \bbR$ bounded, $\bbE \{ \lmb f(Z + 1) - Zf(Z) \} = 0$.

\item Let $\{ X_i \}_{i \in I}$ be a collection of 0/1-valued random variables, and let $(I, E)$ be a dependency graph for $\{ X_i \}_{i \in I}$. Let $p_{ij} = P(X_i = X_j = 1)$, and let $P_W$ denote the probability distribution of $W = \sum X_i$. Then
\begin{equation*} \lVert P_W - \text{Poisson}(\lmb) \rVert_{TV} \leq \min (3, \lmb^{-1}) \left[ \sum_{i \in I} \sum_{j \in N_i \setminus \{ i\}} p_{ij} + \sum_{i \in I} \sum_{j \in N_i} p_i p_j \right]. \end{equation*}
\end{itemize}


% Weak convergence & Central Limit Theorem
\section*{Weak convergence \& Central Limit Theorem (Lec 16-18)}
\begin{itemize}
\item (Lec 18) \textbf{Slutsky's Theorem}: If $X_n \Rightarrow Z$ and $X_n - Y_n \Rightarrow 0$, then $Y_n \Rightarrow Z$.

\item (Lec 16) Let $\mathcal{C}_b^\infty = \{ f: f: \bbR \rightarrow \bbR, f \text{ has bounded derivatives of all order} \}$.

If $F_n$, $F$ are distribution functions on $\bbR$ such that $F_n \Rightarrow F$, then
\begin{equation*} \int_{-\infty}^\infty f dF_n \rightarrow \int_{-\infty}^\infty f dF \end{equation*}
for all $f \in \mathcal{C}_b^\infty$. The converse is true as well.

\item (Lec 16) \textbf{Lindeberg's CLT}: Let $\{X_{ni}\}$ be a triangular array. Suppose that $\bbE(X_{ni}) = 0$ for all $n$ and $i$, and that $\var X_{ni} = \sg_{ni}^2 < \infty$. Let $S_n = \displaystyle\sum_{i=1}^{k_n} X_{ni}$, $s_n^2 = \displaystyle\sum_{i=1}^{k_n} \sg_{ni}^2 $ (i.e. sum of the $n^{th}$ row and its variance.)

Suppose for every $n$, $\{ X_{ni} \}_{i=1}^{k_n}$ is independent. Suppose also that the Lindeburg condition holds: i.e. for every $\eps > 0$,
\begin{equation*} \lim_{n \rightarrow \infty} \frac{1}{s_n^2} \sum_{i = 1}^{k_n} \int_{ \{ |X_{ni}| > \eps s_n \} } X_{ni}^2 dP = 0. \end{equation*}

Then for every $x \in \bbR$,
\begin{equation*} P\left\{ \frac{S_n}{s_n} \leq x \right\} \rightarrow \Phi(x). \end{equation*}

\item (Lec 17) \textbf{Lyapounov's CLT}: Let $X_{ni}$ be a triangular array such that the $X_{ni}$'s have mean 0, $\bbE |X_{ni}|^{2 + \dlt} < \infty$ for some $\dlt > 0$.

If Lyapunov's condition holds:
\begin{equation*} \lim_{n \rightarrow \infty} \frac{1}{s_n^{2+ \dlt}} \sum_{i = 1}^{k_n} \bbE |X_{ni}|^{2 + \dlt} = 0, \end{equation*}
then $P \left\{ \displaystyle\frac{S_n}{s_n} \leq x \right\}  \rightarrow \Phi(x)$.

\item \textbf{CLT for single variable:} Assume $X_i$'s are i.i.d. with finite mean $\mu$ and finite variance $\sg^2$. Let $S_n = X_1 + \dots + X_n$. Then $\sqrt{n} \left(\dis\frac{S_n}{n} - \mu \right) \stackrel{d}{\goesto} \calN(0, \sg^2)$. Equivalently, $\dis\frac{S_n - n\mu}{\sg / \sqrt{n}} \stackrel{d}{\goesto} \calN(0, 1)$.

\item In trying to prove that the condition for CLT holds (and a direct verification of the conditions doesn't work), consider using truncation or looking at characteristic functions.

\item \textbf{Lindeberg-Feller}: Consider the set-up for Lindeberg CLT where instead of a triangular array we just have $X_1, X_2, \dots$ independent. If this sequence satisfies $\displaystyle\max_{k = 1, \dots, n}\frac{\sg_k^2}{s_n^2} \goesto 0$ as $n \goesto \infty$, then Lindeberg's condition is both necessary and sufficient.

\item (Lec 17) \textbf{Berry-Esseen bounds for CLT approximation:} Assume $X_1, \dots, X_n$ have mean $0$, variance $\sg_i^2$, and $\bbE |X_i|^3 = r_i$ are finite. Then
\begin{equation*} \sup_{-\infty < x < \infty} \left| P \left\{\frac{S_n}{s_n} \leq x \right\} - \Phi(x)\right| \leq \frac{0.78R_n}{s_n^3}, \end{equation*}
where $R_n = \sum_{i=1}^n r_i$. If the $X_i$'s are iid, the RHS is $\dis\frac{0.78 \bbE |X_1|^3}{\sg^3 \sqrt{n}}$.

\end{itemize}

% Characteristic functions
\section*{Characteristic functions (Lec 18-20)}
\begin{itemize}
\item (Lec 18) \textbf{Skorohod's Theorem:} Let $F_n$, $F$ be distribution functions on $\bbR$ such that $F_n \Rightarrow F$. Then there exist $(\Om, \calF, P)$ and random variables $\{Y_n\}$, $Y$ with distribution functions $\{F_n\}$, $F$ such that $Y_n(\om) \rightarrow Y(\om)$ for all $\om$.

\item (Lec 19) \textbf{Continuity Theorem:} $F_n \Rightarrow F$ iff $\phi_n(t) \rightarrow \phi(t)$ for all $t$. (The same is true for MGFs, if they exist.)

\item (Lec 19) \textbf{Definition of tightness:} A family of probabilities $\{\mu_n\}$ on $\bbR$ is \textbf{tight} if for every $\eps > 0$, there exist $a < b$ so that $\mu_n(a, b] > 1 - \eps$ for all $n$. We say that $\{\mu_n\}$ is \textbf{almost compactly supported}.

\item (Lec 19) $\{ \mu_n \}$ is tight iff for every subsequence $\{ n_k \}_{k=1}^\infty$, there exists a further subsequence $\{ n_{k_i} \}_{i=1}^\infty$ and a probability $\mu$ so that $\mu_{n_{k_i}} \Rightarrow \mu$.

\item (Lec 19) \textbf{Cantor's diagonal argument:} Let $\{ X_{ij}\}_{i,j = 1}^\infty$ be a 2D array of real numbers such that each row is bounded. Then there exists a subsequence $\{ n_k\}_{k=1}^\infty$ of $\bbN$ and $\{l_r\}_{r = 1}^\infty$ such that $x_{rn_k} \goesto l_r$ as $k \goesto \infty$ for all $r \in \bbN$.

\item (Lec 19) \textbf{Helly selection theorem:} If $\{F_n \}_{n=1}^\infty$ are any distribution functions on $\bbR$, then there exist monotone, right-continuous $F$ and a subsequence $n_k \nearrow \infty$ such that $F_{n_k}(x) \rightarrow F(x)$ for all points of continuity $x$ of $F$. (If $\{F_n \}$ is tight, the subsequence can be chosen suc that limit $F$ is a distribution function.)

\item (Lec 20) \textbf{Inversion formula:} If $a < b$ with $\mu\{a\} = \mu\{b\} = 0$ (i.e. not atoms of the measure), then 
\begin{equation*} \mu(a, b] = \lim_{T \rightarrow \infty} \frac{1}{2\pi}\int_{-T}^T \frac{e^{-ita} - e^{itb}}{it} \phi(t) dt. \end{equation*}

\item (Lec 20) If $\phi_\mu (t) = \phi_\nu (t)$ for all $t$, then $\mu = \nu$.

\item (Lec 20) \textbf{Fourier transform:} If $\phi(t)$ is integrable, and if $F$ is the corresponding distribution function, then $F$ has density $f(x) = \dis\frac{1}{2\pi}\int_{-\infty}^\infty e^{-itx}\phi(t) dt$.

\item (Lec 20) A weighted average of characteristic functions is still a characteristic function.

\item (Lec 20) \textbf{P\'{o}lya's criteria:} If $\phi$ is continuous, non-negative, even, $\phi(0) = 1$, $\phi$ convex on $(0, \infty)$ and $(-\infty, 0)$, then $\phi$ is a characteristic function.

\item (Lec 20) 2 characteristic functions can agree in a neighborhood of 0 without having the same measure.

\item $\phi_{X+Y} = \phi_X \phi_Y$ does \textbf{not} imply that $X$ and $Y$ are independent. (E.g. $X = Y =$ standard Cauchy.) However, if $X$ and $Y$ and $\bbR^d$-valued random variables, then
\[ \text{$X$ and $Y$ are independent} \quad \Leftrightarrow \quad \bbE \left[ e^{t (X, Y) \cdot (\xi, \eta)} \right] = \bbE \left[ e^{t X \cdot \xi} \cdot \bbE \left[ e^{t (X, Y) \cdot (\xi, \eta)} \right] \right] \]

\end{itemize}

\end{document}